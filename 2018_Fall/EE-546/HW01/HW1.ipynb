{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cmb-panasas2/skchoudh/software_frozen/anaconda27/envs/riboraptor/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/cmb-panasas2/skchoudh/software_frozen/anaconda27/envs/riboraptor/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/cmb-panasas2/skchoudh/software_frozen/anaconda27/envs/riboraptor/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json\n",
    "sns.set_context('paper', font_scale=2)\n",
    "sns.set_style('white')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/wdbc.data', header=None)\n",
    "df = df.drop(columns=[0])\n",
    "output = np.array(df.loc[:, 1])\n",
    "df  = df.drop(columns=[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cmb-panasas2/skchoudh/software_frozen/anaconda27/envs/riboraptor/lib/python3.5/site-packages/ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_matrix = df.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1056474.4596355997"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(df_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized = df_matrix - df_matrix.mean()\n",
    "df_normalized = df_normalized/np.sqrt((df_normalized * df_normalized).sum(axis=1))[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01998381, -0.02344792,  0.02772619, ..., -0.02805213,\n",
       "        -0.0279635 , -0.02811882],\n",
       "       [-0.01797171, -0.01918952,  0.03088423, ..., -0.02683737,\n",
       "        -0.02679866, -0.02687955],\n",
       "       [-0.0208045 , -0.02003544,  0.03357715, ..., -0.03039166,\n",
       "        -0.03033334, -0.03046828],\n",
       "       ...,\n",
       "       [-0.03326271, -0.02483149,  0.03408422, ..., -0.04535005,\n",
       "        -0.04529129, -0.04539676],\n",
       "       [-0.01915379, -0.01510415,  0.03627945, ..., -0.02858672,\n",
       "        -0.02852006, -0.02865213],\n",
       "       [-0.141273  , -0.09747973, -0.03646145, ..., -0.16152542,\n",
       "        -0.16077614, -0.16134172]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem (a), (b)\n",
    "\n",
    "In all of our experiments we will perform 100 random partitions of the data for\n",
    "train/test and then report the final result as the average over these tria\n",
    "\n",
    "\\begin{align*}\n",
    "(\\hat{w}, \\hat{b}) &= \\text{arg} \\min_{w,b} f(\\vec{w}, b) := \\frac{1}{N} \\sum_{i=1}^N -y_i(\\vec{w}^T\\vec{x}_i + b) + \\log(1+ \\exp{(\\vec{w}^T\\vec{x}_i + b)} + \\frac{\\lambda^2}{2N} ||\\vec{w}||_{l_2}^2\n",
    "\\end{align*}\n",
    "\n",
    "Consider the derivatives:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial f(\\vec{w}, b)}{\\partial b} &= \\frac{1}{N} \\sum_{i=1}^N -y_i + \\frac{\\exp{(\\vec{w}^T\\vec{x}_i + b)}}{1+ \\exp{(\\vec{w}^T\\vec{x}_i + b)}}\\\\\n",
    "&= \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{1+ \\exp{(-(\\vec{w}^T\\vec{x}_i + b))}}-y_i\n",
    "\\end{align}\n",
    "\n",
    "For $j \\geq 1$ and learning rate $\\mu$:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial f(\\vec{w}, b)}{\\partial w_j} &= \\frac{1}{N} \\sum_{i=1}^N -y_i(x_j) + \\frac{\\exp{(\\vec{w}^T\\vec{x}_i + b)}}{1+ \\exp{(\\vec{w}^T\\vec{x}_i + b)}}x_j + \\frac{\\lambda}{N} w_j\\\\ \n",
    "&=  \\sum_{i=1}^N \\big(\\frac{1}{1+ \\exp{(-(\\vec{w}^T\\vec{x}_i + b))}}-y_i\\big)x_j + \\frac{\\lambda}{N}w_j\n",
    "\\end{align}\n",
    "\n",
    "Gradient descent:\n",
    "\n",
    "\\begin{align*}\n",
    "b^{(t+1)} &= b^{(t)} - \\frac{\\mu}{N} \\sum_{i=1}^N \\big(\\frac{1}{1+ \\exp{(-(\\vec{w}^T\\vec{x}_i + b))}}-y_i\\big)\\\\\n",
    "w^{(t+1)}_j &= w^{(t)}_j - \\frac{\\mu}{N} \\sum_{i=1}^N \\big(\\frac{1}{1+ \\exp{(-(\\vec{w}^T\\vec{x}_i + b))}}-y_i\\big)x_{ij} + \\frac{\\lambda}{N}w_j^{(t)}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "where $x_{ij}$ refers to the $j^{th}$ feature of the $i^{th}$ sample. In our case.\n",
    "$1 \\leq j \\leq 30$ and $i$ ranges over the number of samples.\n",
    "\n",
    "\n",
    "This looks super confusing, let's resort to matrix notations., We construct an augmented matrix $X$ by appending a column of 1s in the beginning that will capture $b$.\n",
    "\n",
    "$X = \\begin{pmatrix}1 & x_{11} & x_{12} & \\cdots & x_{1M}\\\\\n",
    "1 & x_{21} & x_{22} & \\cdots & x_{2M}\\\\\n",
    "\\vdots\\\\\n",
    "1 & x_{N1} & x_{N2} & \\cdots & x_{NM}\\\\\n",
    "\\end{pmatrix}_{N\\times(M+1)}$\n",
    "\n",
    "Our iteration vector is $\\vec{w}_{1\\times(M+1)} = \\begin{pmatrix} b \\\\ w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_M \\end{pmatrix}_{(M+1)\\times 1}$, thus $X\\vec{w}$ has dimensions $N \\times 1$ which is same as our target vector $\\vec{y}$\n",
    "\n",
    "Let's also define an augmented  vector $\\vec{v} = \\begin{pmatrix} 0 \\\\ w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_M \\end{pmatrix}_{(M+1)\\times 1}$\n",
    "\n",
    "And the update simplifies to:\n",
    "\n",
    "\\begin{align*}\n",
    "\\vec{w}_{(M+1)\\times 1}^{(t+1)} &= \\vec{w}^{(t+1)}_{(M+1)\\times 1} - \\frac{\\mu}{N} X^T(X\\vec{w}^{(t+1)}-\\vec{y}) + \\frac{\\lambda}{M} \\vec{v}^{t}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(X, y, n_train=500):\n",
    "    nrows = X.shape[0]\n",
    "    indices = numpy.random.permutation(nrows)\n",
    "    training_idx, test_idx = indices[:n_train], indices[n_train:]\n",
    "    X_train, X_test = X[training_idx,:], X[test_idx,:]\n",
    "    y_train, y_test = y[training_idx], y[test_idx]\n",
    "    return (X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "N_iter = 100\n",
    "lamda_w = 0.01\n",
    "T = 500\n",
    "\n",
    "for i in range(N_iter):\n",
    "    train_x, train_label, test_x, test_label = split_train_test(df_normalized, output)\n",
    "    X_train.append(train_x)\n",
    "    y_train.append(train_label)\n",
    "    X_test.append(test_x)\n",
    "    y_test.append(test_label)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "\n",
    "def calc_error(z, y_true):\n",
    "    y_predicted = sigmoid(z)\n",
    "    y_predicted[y_predicted>0.5] = 1\n",
    "    y_predicted[y_predicted<=0.5] = 0\n",
    "    return np.mean(np.abs(y_predicted-y_true))\n",
    "\n",
    "\n",
    "def logistic_regression(X, y, num_steps=500, learning_rate=1e-6, regularizer_lambda=0.01):\n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    X = np.hstack((intercept, X))\n",
    "    weights = np.zeros(X.shape[1])\n",
    "    regularization_indicator = np.ones(X.shape[1])\n",
    "    regularization_indicator[0] =  0\n",
    "    regularization_weights = np.multiply(regularization_indicator, weights)\n",
    "    training_errors = []\n",
    "    for step in range(num_steps):\n",
    "        scores = np.dot(X, weights)\n",
    "        predictions = sigmoid(scores)\n",
    "        error = predictions - y\n",
    "        gradient = np.dot(X.T, error)\n",
    "        regularization_weights = np.multiply(regularization_indicator, weights)\n",
    "        regularization = regularizer_lambda * regularization_weights\n",
    "\n",
    "        weights = weights - (learning_rate * gradient) + regularization\n",
    "        \n",
    "        training_error = calc_error(scores, y)\n",
    "        training_errors.append(training_error)\n",
    "        print(training_error)\n",
    "    return weights, training_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.364\n",
      "0.364\n",
      "0.566\n",
      "0.364\n",
      "0.526\n",
      "0.364\n",
      "0.488\n",
      "0.364\n",
      "0.424\n",
      "0.364\n",
      "0.354\n",
      "0.364\n",
      "0.282\n",
      "0.364\n",
      "0.234\n",
      "0.362\n",
      "0.174\n",
      "0.346\n",
      "0.144\n",
      "0.296\n",
      "0.112\n",
      "0.228\n",
      "0.098\n",
      "0.164\n",
      "0.094\n",
      "0.132\n",
      "0.084\n",
      "0.112\n",
      "0.082\n",
      "0.104\n",
      "0.084\n",
      "0.086\n",
      "0.082\n",
      "0.084\n",
      "0.08\n",
      "0.078\n",
      "0.082\n",
      "0.078\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.084\n",
      "0.084\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.08\n",
      "0.08\n",
      "0.08\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.082\n",
      "0.082\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.086\n",
      "0.086\n",
      "0.086\n",
      "0.086\n",
      "0.086\n",
      "0.086\n",
      "0.084\n",
      "0.084\n",
      "0.082\n",
      "0.08\n",
      "0.08\n",
      "0.08\n",
      "0.08\n",
      "0.08\n",
      "0.08\n",
      "0.08\n",
      "0.08\n",
      "0.08\n",
      "0.08\n",
      "0.078\n",
      "0.078\n",
      "0.078\n",
      "0.078\n",
      "0.078\n",
      "0.078\n",
      "0.078\n",
      "0.078\n",
      "0.078\n",
      "0.078\n",
      "0.08\n",
      "0.08\n",
      "0.078\n",
      "0.078\n",
      "0.078\n",
      "0.078\n",
      "0.078\n",
      "0.08\n",
      "0.08\n",
      "0.08\n",
      "0.08\n",
      "0.08\n",
      "0.08\n",
      "0.08\n",
      "0.08\n",
      "0.08\n",
      "0.08\n",
      "0.08\n",
      "0.08\n",
      "0.08\n",
      "0.08\n",
      "0.08\n",
      "0.08\n",
      "0.08\n",
      "0.08\n",
      "0.08\n",
      "0.08\n",
      "0.08\n",
      "0.082\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.084\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n",
      "0.082\n"
     ]
    }
   ],
   "source": [
    "weights, error = logistic_regression(X_train[0], y_train[0], num_steps=500, learning_rate=0.01,\n",
    "                                     regularizer_lambda=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_observations = 1000\n",
    "\n",
    "x1 = np.random.multivariate_normal([0, 0], [[1, .75],[.75, 1]], num_observations)\n",
    "x2 = np.random.multivariate_normal([1, 4], [[1, .75],[.75, 1]], num_observations)\n",
    "simulated_separableish_features = np.vstack((x1, x2)).astype(np.float32)\n",
    "simulated_labels = np.hstack((np.zeros(num_observations),\n",
    "                              np.ones(num_observations)))\n",
    "simulated_separableish_features.shape\n",
    "weights = np.zeros(simulated_separableish_features.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(simulated_separableish_features, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem (c)\n",
    "Now that we have normalized data, partition it into train/test sets at random 100 times\n",
    "as discussed earlier. In each trial learn the model by solving (1) with λ = 0.01. To\n",
    "due this run gradient descent for T = 500 iterations and then use the trained model to\n",
    "make predictions on the test data and calculate the average error (average number of\n",
    "miss-classified patients on the test data) for each trial. Report the average over the 100\n",
    "trials. The value of the step size you use does not matter too much. However, make sure\n",
    "that the algorithm has converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem (d)\n",
    "\n",
    "Perform the experiment with the same step size you used before but now report the\n",
    "number of iterations it takes to get to an accuracy of 10−6\n",
    "calculated via the first iteration t when the following inequality holds\n",
    "k∇f(wt, bt)k2 ≤ 10−6(1 + |f(wt, bt)|)\n",
    "\n",
    "The number you should report is the average of this number over the 100 trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem (e)\n",
    "\n",
    "Perform the experiment of part (d) but now add a momentum term (1) using the heavy\n",
    "ball method and (2) using Nesterov’s accelerated scheme. In both cases keep the same\n",
    "step size as part (d) but fine tune the momentum parameter to get the smallest number\n",
    "of iterations for convergence based on the stopping criteria (2) (again averaged over the\n",
    "100 trials). Draw the convergence of the three algorithms gradient descent, heavy ball,\n",
    "Nesterov’s accelerated scheme for one trial. That is, draw the ratio\n",
    "k∇f(wt, bt)k2`2(1 + |f(wt, bt)|) as a function of the iteration number t = 1, 2, . . . , 500. Which algorithm would you use\n",
    "and why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (riboraptor_clean)",
   "language": "python",
   "name": "riboraptor_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
