%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Structured General Purpose Assignment
% LaTeX Template
%
% This template has been downloaded from:
% http://www.latextemplates.com
%
% Original author:
% Ted Pavlic (http://www.tedpavlic.com)
%
% Note:
% The \lipsum[#] commands throughout this template generate dummy text
% to fill the template out. These commands should all be removed when 
% writing assignment content.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%       PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage{graphicx} % Required to insert images
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{subcaption}
\usepackage{mathtools}
\usepackage{tikz}


\usepackage{empheq}

\newcommand*\widefbox[1]{\fbox{\hspace{2em}#1\hspace{2em}}}
\usepackage{amsmath}

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in 

\linespread{1.1} % Line spacing

% Set up the header and footer
\pagestyle{fancy}
\lhead{\hmwkAuthorName} % Top left header
\chead{\hmwkClass\ : \hmwkTitle} % Top center header
\rhead{\firstxmark} % Top right header
\lfoot{\lastxmark} % Bottom left footer
\cfoot{} % Bottom center footer
\rfoot{Page\ \thepage\ of\ \pageref{LastPage}} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule

\setlength\parindent{0pt} % Removes all indentation from paragraphs

%----------------------------------------------------------------------------------------
%       DOCUMENT STRUCTURE COMMANDS
%       Skip this unless you know what you're doing
%----------------------------------------------------------------------------------------

% Header and footer for when a page split occurs within a problem environment
\newcommand{\enterProblemHeader}[1]{
\nobreak\extramarks{#1}{#1 continued on next page\ldots}\nobreak
\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
}

% Header and footer for when a page split occurs between problem environments
\newcommand{\exitProblemHeader}[1]{
\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
\nobreak\extramarks{#1}{}\nobreak
}

\setcounter{secnumdepth}{0} % Removes default section numbers
\newcounter{homeworkProblemCounter} % Creates a counter to keep track of the number of problems

\newcommand{\homeworkProblemName}{}
\newenvironment{homeworkProblem}[1][Problem \arabic{homeworkProblemCounter}]{ % Makes a new environment called homeworkProblem which takes 1 argument (custom name) but the default is "Problem #"
\stepcounter{homeworkProblemCounter} % Increase counter for number of problems
\renewcommand{\homeworkProblemName}{#1} % Assign \homeworkProblemName the name of the problem
\section{\homeworkProblemName} % Make a section in the document with the custom problem count
\enterProblemHeader{\homeworkProblemName} % Header and footer within the environment
}{
\exitProblemHeader{\homeworkProblemName} % Header and footer after the environment
}

\newcommand{\problemAnswer}[1]{ % Defines the problem answer command with the content as the only argument
\noindent\framebox[\columnwidth][c]{\begin{minipage}{0.98\columnwidth}#1\end{minipage}} % Makes the box around the problem answer and puts the content inside
}

\newcommand{\homeworkSectionName}{}
\newenvironment{homeworkSection}[1]{ % New environment for sections within homework problems, takes 1 argument - the name of the section
\renewcommand{\homeworkSectionName}{#1} % Assign \homeworkSectionName to the name of the section from the environment argument
\subsection{\homeworkSectionName} % Make a subsection with the custom name of the subsection
\enterProblemHeader{\homeworkProblemName\ [\homeworkSectionName]} % Header and footer within the environment
}{
\enterProblemHeader{\homeworkProblemName} % Header and footer after the environment
}
   
%----------------------------------------------------------------------------------------
%       NAME AND CLASS SECTION
%----------------------------------------------------------------------------------------

\newcommand{\hmwkTitle}{Assignment\ \#4 } % Assignment title
\newcommand{\hmwkDueDate}{Monday,\ November\ 9,\ 2015} % Due date
\newcommand{\hmwkClass}{CSCI-567} % Course/class
\newcommand{\hmwkClassTime}{} % Class/lecture time
\newcommand{\hmwkAuthorName}{Saket Choudhary} % Your name
\newcommand{\hmwkAuthorID}{2170058637} % Teacher/lecturer
%----------------------------------------------------------------------------------------
%       TITLE PAGE
%----------------------------------------------------------------------------------------

\title{
\vspace{2in}
\textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
\normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
\vspace{0.1in}\large{\textit{\hmwkClassTime}}
\vspace{3in}
}

\author{\textbf{\hmwkAuthorName} \\
        \textbf{\hmwkAuthorID}
        }
\date{} % Insert date here if you want it to appear below your name

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle

%----------------------------------------------------------------------------------------
%       TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

%\setcounter{tocdepth}{1} % Uncomment this line if you don't want subsections listed in the ToC

\newpage
\tableofcontents
\newpage

\begin{homeworkProblem}[Problem 1] % Custom section title
\begin{homeworkSection}{\homeworkProblemName: ~(a)  Gradient Calculation}
\problemAnswer{
    $$
      L(y_i, \hat{y_i}) = \log(1+\exp(-y_i\hat{y_i}))
    $$
    \begin{align*}
        g_i &= \frac{\partial L(y_i, \hat{y_i})}{\partial \hat{y_i}}\\
        \Aboxed{g_i   &= \frac{-y_i\exp(-y_i\hat{y_i})}{1+\exp(-y_i\hat{y_i})}}
    \end{align*}
}
\end{homeworkSection}

\begin{homeworkSection}{\homeworkProblemName: ~(b)  Weak Learner Section}
\problemAnswer{

\begin{eqnarray*}
	h^* = \min_{h \in H} \big( \min_{\gamma \in R} \sum_{i=1}^n (-g_i-\gamma h(x_i))^2 \big)\\
	\implies 
    \frac{\partial h^* }{\partial \gamma} =0\\
   \implies 2\sum_{i=1}^n (-g_i-\gamma h(x_i))(-h(x_i)) = 0\\
    \Aboxed{\hat{h} = -\frac{\sum_{i=1}^n g_ih(x_i)}{\sum_{i=1}^n h(x_i)^2}}
\end{eqnarray*}

Also check if it is indeed minimum with a second derivative test:
\begin{eqnarray*}
\frac{\partial^2h^* }{\partial \gamma^2} = 2 \sum_{i=1}^n h(x_i)^2 > 0
\end{eqnarray*}

Since the second derivative is positive definite, $\hat{gamma}$ is indeed where the minima occurs.
}
\end{homeworkSection}

\begin{homeworkSection}{\homeworkProblemName: ~(c)  Step Size Selection}
    \problemAnswer{
        $$
	        \alpha^* = \arg \min_{\alpha \in R} \sum_1^n L(y_i, \hat{y_i}+\alpha h^*(x_i))         
        $$
        Newton's approximation:

        $$
        \alpha_1 = \alpha_0 - \frac{f'(\alpha_0)}{f''(\alpha_0)}
        $$
        We start from $\alpha_0=0$
        and hence:
        
		\begin{align*}
		f(\alpha_0) &=   \sum_{i=1}^n \log(1+\exp(-y_i\hat{y_i})) \\
		f'(\alpha) & = \sum_{i=1}^n \frac{\partial L}{\partial \alpha }\\
		 &= \sum_{i=1}^n \frac{-y_i h^*(x_i)\exp{(-y_i(\hat{y_i}+\alpha h^*(x_i)))}}{1+\exp{(-y_i(\hat{y_i}+\alpha h^*(x_i)))}}\\
		 \Aboxed{f'(\alpha=\alpha_0)&=  -\sum_{i=1}^n \frac{y_ih^*(x_i)\exp(-y_i\hat{y_i})}{1+\exp(-y_i\hat{y_i})}}
		\end{align*}  
		      
		And,
		
		\begin{align*}
		f''(\alpha) &= \sum_{i=1}^n \frac{\partial^2 L}{\partial \alpha^2}\\
		&= \sum_{i=1}^n \frac{\big\{\big(1+\exp{(-y_i(\hat{y_i}+\alpha h^*(x_i)))}\big)(y_ih^*(x_i))^2  +y_ih^*(x_i) \big\} \exp{(-y_i(\hat{y_i}+\alpha h^*(x_i)))}}{(1+\exp{(-y_i(\hat{y_i}+\alpha h^*(x_i)))})^2}\\
		\Aboxed{f''(\alpha_0)&= \sum_{i=1}^n \frac{\big\{\big(1+\exp{(-y_i\hat{y_i})}\big)(y_ih^*(x_i))^2  +y_ih^*(x_i) \big\} \exp{(-y_i\hat{y_i})}}{(1+\exp{(-y_i\hat{y_i})})^2}}\\
		\end{align*}
		      Thus,
		      $$
		      %\alpha_1 = \frac{\sum_{i=1}^n(\log(1+\exp(-y_i\hat{y_i})))}{\sum_{i=1}^n \frac{-y_ih^*(x_i)\exp(-y_i\hat{y_i})}{1+\exp(-y_i\hat{y_i})}}
		      \boxed{\alpha_1 = \frac{\sum_{i=1}^n \frac{y_ih^*(x_i)\exp(-y_i\hat{y_i})}{1+\exp(-y_i\hat{y_i})}}{\sum_{i=1}^n \frac{\big\{\big(1+\exp{(-y_i\hat{y_i})}\big)(y_ih^*(x_i))^2  +y_ih^*(x_i) \big\} \exp{(-y_i\hat{y_i})}}{(1+\exp{(-y_i\hat{y_i})})^2}}}
		      $$
    }

\end{homeworkSection}


\end{homeworkProblem}

\begin{homeworkProblem}[Problem 2]
    \begin{homeworkSection}{\homeworkProblemName: ~(a)}
        \problemAnswer{
            Primal form:
                %\begin{empheq}[box=\widefbox]
                	\begin{align*} 
                    \min_w  ||w||^2\\
                    \text{such that } |y_i-(w^Tx_i+b)| \leq \epsilon
	                \end{align*}
                %\end{empheq}
            
        }
    \end{homeworkSection}
    
    \begin{homeworkSection}{\homeworkProblemName: ~(b)}
        \problemAnswer{ 
            \begin{align*}
            \min_{_w,\epsilon_i}  \frac{1} {2}||w||^2 +  C\sum_i \epsilon_i\\
            \text{such that } (w^Tx_i+b) - y_i \leq n_i + \epsilon_i \text{ (positive deviation)}\\
             \text{ and } y_i-(w^Tx_i+b) \leq {p_i}+ \epsilon_i \text{ (negative deviation)}\\
	         n_i \geq 0\\
	         p_i \geq 0
            \end{align*}
            
            Also,
            the slackness loss needs further constraints:
            \begin{align*}
            n_i &= \begin{cases}
            0 & |n_i| < \epsilon_i,\\
            |n_i|-\epsilon & otherwise
            \end{cases}\\
            p_i &= \begin{cases}
            0 & |p_i| < \epsilon_i,\\
            |p_i|-\epsilon & otherwise
            \end{cases}
            \end{align*}
            \begin{center}
            	

            \begin{tikzpicture}
            \draw[->] (-3,0) -- (3.2,0) node[right] {$x$};
            \draw[->] (0,-3) -- (0,3.2) node[above] {$y$};
            \draw[scale=0.5,domain=1:3,smooth,variable=\x,blue] plot ({\x},{\x-1});
           \draw[scale=0.5,domain=-3:-1,smooth,variable=\x,blue] plot ({\x},{-\x-1});
           \filldraw[black] (-0.5,0) circle (2pt) node[anchor=south]   {$-\epsilon_i$};
            \filldraw[black] (0.5,0) circle (2pt) node[anchor=south]   {$\epsilon_i$};
            \end{tikzpicture}
            \end{center}
            
            So essentially $n_i,p_i$ are non zero, only above the two blue lines
        }
    \end{homeworkSection}
    \begin{homeworkSection}{\homeworkProblemName: ~(c)}
        \problemAnswer{
\begin{align*}
L  &= \frac{1}{2}||w||^2 + C\sum_i (p_i+n_i)\\
 &- \sum_i(\lambda_ip_i+\lambda_i'n_i)\\ 
 &-\sum_i \alpha_i(\epsilon+ p_i - (y_i-(w^Tx_i+b)))\\
 &- \sum_i \beta_i (\epsilon+ n_i + (y_i-(w^Tx_i+b)))
\end{align*}

{\centering{\textbf{Conditions	}:}}
\begin{align*}
\alpha_i &\geq 0\\
\beta_i &\geq 0\\
\lambda_i &\geq 0\\
\lambda_i' &\geq 0
\end{align*}


Dual Form(all summations are from 1 to n)::
\begin{align*}
\Delta_w L &=0 \\
&= w - \sum_i \alpha_i x_i + \sum_i \beta_i x_i = 0\\
&= w-\sum_i( \alpha_i-\beta_i	) x_i=0\\
\Delta_b L &=0\\
&= \sum_i \alpha_i - \sum_i \beta_i = 0\\
\Delta_{p_i} L &= 0\\
&= C-\sum \lambda_i -\sum_i \alpha_i   =0\\
\Delta_{n_i} L &= 0\\
&= C-\sum \lambda'_i -\sum_i \beta_i   =0\\
\end{align*}
}
\problemAnswer{

Thus, $w$ is given by:
$$
w = \sum_i \alpha_i x_i - \sum_i \beta_i x_i
$$

depends only on the support vectors.


This reduces the optimisation to:

\begin{align*}
\max f &= \frac{1}{2} \sum_{i,j} (\alpha_i-\beta_i)x_i^Tx_j(\alpha_j-\beta_j) + p_i(C-\sum_i \lambda_i - \sum_i \alpha_i)\\ 
&+ n_i(C+\sum_i \lambda'_i -\sum_i \beta_i)\\
&+ \epsilon(-\sum_i \alpha_i - \sum_i \beta_i)\\ 
&+ \sum_i y_i(\alpha_i-\beta_i)  - \sum_i(\alpha_iw^Tx_i-\beta_iw^Tx_i)\\
&= -\frac{1}{2} \sum_{i,j} (\alpha_i-\beta_i)x_i^Tx_j(\alpha_j-\beta_j)  - \epsilon (\sum_i(\alpha_i+\beta_i))\\
&+ \sum_i y_i(\alpha_i-\beta_i)\\
\text{such that} \sum_i(\alpha_i-\beta_i) &= 0 \\
\text{and } \alpha_i,\beta_i &\in [0,C]
\end{align*}

        }
    \end{homeworkSection}
    \begin{homeworkSection}{\homeworkProblemName: ~(d)}
\problemAnswer{
	Using Kernel transformation, we simply replace $x_i^Tx_j$ with $k(x_i, x_j)$:
	$$
	w = \sum_i(\alpha_i-\beta_i)\phi(x_i)
	$$	
	this happens because $x_i'x_j$ gets mapped onto by an equivalent kernel function $k(x_i,x_j)=\phi^T(x_i)\phi(x_j)$
	and the objective function is:
	$$
	\max_f = -\frac{1}{2} \sum_{i,j} (\alpha_i-\beta_i)k(x_i, x_j)(\alpha_j-\beta_j)  - \epsilon (\sum_i(\alpha_i+\beta_i)) + \sum_i y_i(\alpha_i-\beta_i)
	$$
	}
	
    \end{homeworkSection}
\end{homeworkProblem}

\begin{homeworkProblem}[Problem 3.3]
	\begin{homeworkSection}{\homeworkProblemName: ~(a)}
		\problemAnswer{
			
			\begin{center}
			%	\begin{table}
					
			\begin{tabular}{|c|c|c|c|c|c|c|}
				\hline
C	& Tr. Dataset 1(s) & Tr. Dataset 2(s) & Training Dataset 3(s)	& CV Accuracy & Avg. time(s) \\
\hline
$4^{-6}$=0.000244 & 0.606156 & 0.400791 & 0.346078 & 0.578976 & 0.451 \\
\hline
$4^{-5}$=0.000977 & 0.379495 & 0.477559 & 0.498054 & 0.907001 & 0.451\\
\hline
$4^{-4}$=0.003906 & 0.529940 & 0.495841 & 0.534946 & 0.926001 & 0.520\\
\hline
$4^{-3}$=0.015625 & 0.516236 & 0.577324 & 0.561874 & 0.935501 & 0.551\\
\hline
$4^{-2}$=0.062500 & 0.512287 & 0.529517 & 0.554510 & 0.945006 & 0.532\\
\hline
$4^{-1}$=0.250000 & 0.630195 & 0.663459 & 0.657651 & 0.943010 & 0.650\\
\hline 
$4^{0}$=1.000000 & 0.746649 & 0.601710 & 0.563063 & 0.939003 & 0.637\\
\hline
$4^{1}$=4.000000 & 0.633370 & 0.572011 & 0.595552 & 0.942501 & 0.600\\
\hline
$4^2$=16.000000 & 0.674677 & 0.681010 & 0.698041 & 0.943503 & 0.684\\
\hline
			\end{tabular}
			\text{ The time shown$(t)$ is in seconds for three partitions, the last column being the average time}
			%\end{table}
			\end{center}
			
			As seen from the table. the time seems to increase with $C$ and the $CV$ increases too.
			
			\begin{itemize}
				\item \textbf{Increasing $C$ leads to increase in runtime}\\
				The larger the value of $C$, the more is the penalisation and hence smaller the $\epsilon_i$ would be, this causes the lower bound in input `quadprog` to increase in size (since $A.x \leq b$ and in this case $\epsilon$ is included in vector $x$, so the search space for $A$ increases, since $|x|$ is  small, when $C$ is large!)
				\item \textbf{Increasing $C$ leads to higher training accuracy}\\
				$C$ determines the tradeoff between objective function complexity and the overall loss. When $C$ is small, there are chances of overfitting, this is evident from low $CV$ values for lower $C$(because the generalisation error is high, and this is where cross validation is helpful) and hence increasing $C$ helps overcome the problem of over-fitting and the generalization error lowers(training accuracy increases)
			\end{itemize}
			
			
			

			}
	\end{homeworkSection}
	\begin{homeworkSection}{\homeworkProblemName: ~(b)}
		\problemAnswer{
Based on \textbf{highest cross validation accuracy}. $C=4^2$ (maximum training accuracy = 94.3503\%)
			}
	\end{homeworkSection}
	
	\begin{homeworkSection}{\homeworkProblemName: ~(c)}
		\problemAnswer{
		With $C=16$, test accuracy = $94.35\%$ (it's pretty close to the training accuracy itself)
		}
	\end{homeworkSection}
	
\end{homeworkProblem}

\begin{homeworkProblem}[Problem 3.4]
	\begin{homeworkSection}{\homeworkProblemName: ~(a)}
		\problemAnswer{
			Platform Used: $Ubuntu\ 12.04,\ x86\_64$\\
			`libsvm` gives $96.55\%$ as its accuracy which is pretty close to  94.35\% that my code gives. \textbf{Hence the cross validation accuracy is not exactly the same, and `libsvm` performs better, by a margin of 2.2786\%}
			\begin{center}
				\begin{tabular}{|c|c|c|c}
					\hline
					C & Avg. Training Time & CV Accuracy\\
					\hline
					$4^{-6}$ & 0.829624 & 0.5575 \\
					\hline
					$4^{-5}$ & 0.827846 & 0.5575\\
					\hline
					$4^{-4}$ & 0.831586 & 0.5575\\
					\hline
					$4^{-3}$ & 0.831599 & 0.7295\\
					\hline
					$4^{-2}$ & 0.649459 & 0.9195\\
					\hline
					$4^{-1}$ & 0.425499 & 0.934\\
					\hline 
					$4^{0}$ & 0.281219 & 0.949\\
					\hline
					$4^{1}$ & 0.210071 & 0.955\\
					\hline
					$4^2$ & 0.201989 & 0.9655\\
					\hline
				\end{tabular}
						\end{center}
			}
	\end{homeworkSection}
	
		\begin{homeworkSection}{\homeworkProblemName: ~(b)}
			\problemAnswer{
				`libsvm` is around 3 times faster in the worst case.
		
			}
		\end{homeworkSection}
		
		
\end{homeworkProblem}


\begin{homeworkProblem}[Problem 3.5]
	\begin{homeworkSection}{\homeworkProblemName: ~(a) Polynomial Kernel}
		\problemAnswer{
		\begin{center}
			
			\begin{tabular}{|c|c|c|c|}
				\hline
				C & $degree$ & Avg. Training Time(s) & CV Accuracy(\%)\\
				\hline
				0.015625 & 1.000000 & 0.652999 & 70.100000\\
				\hline
				0.015625 & 2.000000 & 0.658651 & 55.750000\\
				\hline
				0.015625 & 3.000000 & 0.658636 & 55.750000\\
				\hline
				0.062500 & 1.000000 & 0.513652 & 91.750000\\
				\hline
				0.062500 & 2.000000 & 0.587962 & 86.550000\\
				\hline
				0.062500 & 3.000000 & 0.655689 & 76.900000\\
				\hline
				0.250000 & 1.000000 & 0.334359 & 92.800000\\
				\hline
				0.250000 & 2.000000 & 0.417291 & 92.550000\\
				\hline
				0.250000 & 3.000000 & 0.504703 & 91.800000\\
				\hline
				1.000000 & 1.000000 & 0.230170 & 93.850000\\
				\hline
				1.000000 & 2.000000 & 0.273486 & 94.700000\\
				\hline
				1.000000 & 3.000000 & 0.335671 & 94.350000\\
				\hline
				4.000000 & 1.000000 & 0.181141 & 94.400000\\
				\hline
				4.000000 & 2.000000 & 0.197570 & 95.100000\\
				\hline
				4.000000 & 3.000000 & 0.230603 & 95.700000\\
				\hline
				16.000000 & 1.000000 & 0.165130 & 94.450000\\
				\hline
				16.000000 & 2.000000 & 0.172796 & 96.100000\\
				\hline
				16.000000 & 3.000000 & 0.197574 & 96.450000\\
				\hline
				64.000000 & 1.000000 & 0.186566 & 94.150000\\
				\hline
				64.000000 & 2.000000 & 0.170041 & 96.600000\\
				\hline
				64.000000 & 3.000000 & 0.184937 & 96.550000\\
				\hline
				256.000000 & 1.000000 & 0.288991 & 94.050000\\
				\hline
				256.000000 & 2.000000 & 0.173684 & 96.000000\\
				\hline
				256.000000 & 3.000000 & 0.184054 & 96.300000\\
				\hline
				1024.000000 & 1.000000 & 1.013039 & 94.350000\\
				\hline
				1024.000000 & 2.000000 & 0.223741 & 95.700000\\
				\hline
				1024.000000 & 3.000000 & 0.184358 & 96.300000\\
				\hline
				4096.000000 & 1.000000 & 4.086584 & 94.400000\\
				\hline
				4096.000000 & 2.000000 & 0.204906 & 95.400000\\
				\hline
				4096.000000 & 3.000000 & 0.183501 & 96.250000\\
				\hline
				16384.000000 & 1.000000 & 21.828099 & 94.400000\\
				\hline
				16384.000000 & 2.000000 & 0.206865 & 95.400000\\
				\hline
				16384.000000 & 3.000000 & 0.183685 & 96.250000\\
				\hline
			\end{tabular}
			
		\end{center}
					Polynomial Kernel maximum train accuracy: 96.600000\\
					Polynomial Kernel optimal C: 64\\
					Polynomial Kernel optimal degree: 2\\
					Polynomal Kernel test accuracy(\%): 95.150000\\
		}
	\end{homeworkSection}
	
	\begin{homeworkSection}{\homeworkProblemName: ~(b) RBF Kernel}
		\problemAnswer{
				
				\footnotesize
				\begin{center}
					\begin{tabular}{|c|c|c|c|}
						\hline
						C & $\gamma$ & Training Time(s) & CV (\%)\\
						\hline 
						0.015625 & 0.000061 & 0.799739 & 55.750000\\
						\hline
						0.015625 & 0.000244 & 0.799145 & 55.750000\\
						\hline
						0.015625 & 0.000977 & 0.798638 & 55.750000\\
						\hline
						0.015625 & 0.003906 & 0.801114 & 55.750000\\
						\hline
						0.015625 & 0.015625 & 0.802529 & 64.750000\\
						\hline
						0.015625 & 0.062500 & 0.805714 & 73.750000\\
						\hline
						0.015625 & 0.250000 & 0.822912 & 55.750000\\
						\hline
						0.062500 & 0.000061 & 0.799229 & 55.750000\\
						\hline
						0.062500 & 0.000244 & 0.798608 & 55.750000\\
						\hline
						0.062500 & 0.000977 & 0.799373 & 55.750000\\
						\hline
						0.062500 & 0.003906 & 0.800940 & 83.350000\\
						\hline
						0.062500 & 0.015625 & 0.648832 & 91.200000\\
						\hline
						0.062500 & 0.062500 & 0.634587 & 91.850000\\
						\hline
						0.062500 & 0.250000 & 0.824124 & 63.600000\\
						\hline
						0.250000 & 0.000061 & 0.798746 & 55.750000\\
						\hline
						0.250000 & 0.000244 & 0.799292 & 55.750000\\
						\hline
						0.250000 & 0.000977 & 0.799286 & 86.350000\\
						\hline
						0.250000 & 0.003906 & 0.587110 & 92.000000\\
						\hline
						0.250000 & 0.015625 & 0.426160 & 93.200000\\
						\hline
						0.250000 & 0.062500 & 0.423444 & 94.750000\\
						\hline
						0.250000 & 0.250000 & 0.713477 & 92.300000\\
						\hline
						1.000000 & 0.000061 & 0.800186 & 55.750000\\
						\hline
						1.000000 & 0.000244 & 0.797172 & 86.900000\\
						\hline
						1.000000 & 0.000977 & 0.571743 & 91.850000\\
						\hline
						1.000000 & 0.003906 & 0.381565 & 93.050000\\
						\hline
						1.000000 & 0.015625 & 0.281917 & 94.650000\\
						\hline
						1.000000 & 0.062500 & 0.285756 & 96.150000\\
						\hline
						1.000000 & 0.250000 & 0.568394 & 96.100000\\
						\hline
						4.000000 & 0.000061 & 0.798443 & 86.950000\\
						\hline
						4.000000 & 0.000244 & 0.569461 & 91.850000\\
						\hline
						4.000000 & 0.000977 & 0.371507 & 93.000000\\
						\hline
						4.000000 & 0.003906 & 0.259033 & 94.250000\\
						\hline
						4.000000 & 0.015625 & 0.203123 & 95.550000\\
						\hline
						4.000000 & 0.062500 & 0.230975 & 96.650000\\
						\hline
						4.000000 & 0.250000 & 0.539952 & 96.100000\\
						\hline
						16.000000 & 0.000061 & 0.570222 & 91.850000\\
						\hline
						16.000000 & 0.000244 & 0.369962 & 93.050000\\
						\hline
						16.000000 & 0.000977 & 0.261383 & 93.950000\\
						\hline
						16.000000 & 0.003906 & 0.203001 & 95.050000\\
						\hline
						16.000000 & 0.015625 & 0.195201 & 96.500000\\
						\hline
						16.000000 & 0.062500 & 0.225650 & 96.900000\\
						\hline
						16.000000 & 0.250000 & 0.541735 & 95.950000\\
						\hline
						
					\end{tabular}
					\begin{tabular}{|c|c|c|c|}
						\hline
						C & $\gamma$ & Training Time(s) & CV(\%)\\
						\hline
						64.000000 & 0.000061 & 0.369279 & 93.050000\\
						\hline
						64.000000 & 0.000244 & 0.262142 & 93.950000\\
						\hline
						64.000000 & 0.000977 & 0.210888 & 94.750000\\
						\hline
						64.000000 & 0.003906 & 0.191527 & 94.950000\\
						\hline
						64.000000 & 0.015625 & 0.183208 & 96.650000\\
						\hline
						64.000000 & 0.062500 & 0.229521 & 96.550000\\
						\hline
						64.000000 & 0.250000 & 0.541598 & 95.950000\\
						\hline
						256.000000 & 0.000061 & 0.257230 & 93.900000\\
						\hline
						256.000000 & 0.000244 & 0.214556 & 94.700000\\
						\hline
						256.000000 & 0.000977 & 0.191176 & 94.900000\\
						\hline
						256.000000 & 0.003906 & 0.188024 & 96.450000\\
						\hline
						256.000000 & 0.015625 & 0.186326 & 96.500000\\
						\hline
						256.000000 & 0.062500 & 0.224757 & 96.350000\\
						\hline
						256.000000 & 0.250000 & 0.542700 & 95.950000\\
						\hline
						1024.000000 & 0.000061 & 0.212776 & 94.600000\\
						\hline
						1024.000000 & 0.000244 & 0.191017 & 94.700000\\
						\hline
						1024.000000 & 0.000977 & 0.208193 & 95.050000\\
						\hline
						1024.000000 & 0.003906 & 0.225442 & 96.600000\\
						\hline
						1024.000000 & 0.015625 & 0.194065 & 96.400000\\
						\hline
						1024.000000 & 0.062500 & 0.224793 & 96.350000\\
						\hline
						1024.000000 & 0.250000 & 0.542050 & 95.950000\\
						\hline
						4096.000000 & 0.000061 & 0.192989 & 94.550000\\
						\hline
						4096.000000 & 0.000244 & 0.210952 & 94.750000\\
						\hline
						4096.000000 & 0.000977 & 0.271062 & 96.250000\\
						\hline
						4096.000000 & 0.003906 & 0.277291 & 96.250000\\
						\hline
						4096.000000 & 0.015625 & 0.187257 & 96.350000\\
						\hline
						4096.000000 & 0.062500 & 0.224741 & 96.350000\\
						\hline
						4096.000000 & 0.250000 & 0.541835 & 95.950000\\
						\hline
						16384.000000 & 0.000061 & 0.241238 & 94.350000\\
						\hline
						16384.000000 & 0.000244 & 0.310596 & 94.950000\\
						\hline
						16384.000000 & 0.000977 & 0.401110 & 96.600000\\
						\hline
						16384.000000 & 0.003906 & 0.330598 & 96.250000\\
						\hline
						16384.000000 & 0.015625 & 0.187273 & 96.350000\\
						\hline
						16384.000000 & 0.062500 & 0.224825 & 96.350000\\
						\hline
						16384.000000 & 0.250000 & 0.542722 & 95.950000\\
						\hline
					\end{tabular}
					
				\end{center}
				RBF Kernel maximum train accuracy: \boxed{96.900000}\\
				RBF Kernel optimal $\gamma$: 0.062500\\
				RBF Kernel optimal $C$: 16\\
				RBF Kernel test accuracy(\%): 96.500000\\
				
				
		
		}
	\end{homeworkSection}
	\problemAnswer{
		Summary:\\

		Polynomial Kernel maximum train accuracy: 96.600000\\
		Polynomial Kernel optimal C: 64\\
		Polynomial Kernel optimal degree: 2\\
		Polynomal Kernel test accuracy(\%): 95.150000\\
	
		RBF Kernel maximum train accuracy: \boxed{96.900000}\\
		RBF Kernel optimal $\gamma$: 0.062500\\
		RBF Kernel optimal $C$: 16\\
		RBF Kernel test accuracy(\%): 96.500000\\
		
		\textbf{Hence, Best Kernel: \boxed{RBF}(based on maximum train accuracy) and the best choice
		for RBF kernels 'C' and $\gamma$ are 16 and 0.0625 respectively}(incidentally, also gives a higher test accuracy)

		
		}
	
\end{homeworkProblem}
\end{document}
