\documentclass{article}

\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage{graphicx} % Required to insert images
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template

\usepackage{amsmath}
\usepackage{tikz}
\usetikzlibrary{trees}

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in 

\linespread{1.1} % Line spacing

% Set up the header and footer
\pagestyle{fancy}
\lhead{\hmwkAuthorName} % Top left header
\chead{\hmwkClass\ : \hmwkTitle} % Top center header
\rhead{\firstxmark} % Top right header
\lfoot{\lastxmark} % Bottom left footer
\cfoot{} % Bottom center footer
\rfoot{Page\ \thepage\ of\ \pageref{LastPage}} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule

\setlength\parindent{0pt} % Removes all indentation from paragraphs

%----------------------------------------------------------------------------------------
%       DOCUMENT STRUCTURE COMMANDS
%       Skip this unless you know what you're doing
%----------------------------------------------------------------------------------------

% Header and footer for when a page split occurs within a problem environment
\newcommand{\enterProblemHeader}[1]{
\nobreak\extramarks{#1}{#1 continued on next page\ldots}\nobreak
\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
}

% Header and footer for when a page split occurs between problem environments
\newcommand{\exitProblemHeader}[1]{
\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
\nobreak\extramarks{#1}{}\nobreak
}

\setcounter{secnumdepth}{0} % Removes default section numbers
\newcounter{homeworkProblemCounter} % Creates a counter to keep track of the number of problems

\newcommand{\homeworkProblemName}{}
\newenvironment{homeworkProblem}[1][Problem \arabic{homeworkProblemCounter}]{ % Makes a new environment called homeworkProblem which takes 1 argument (custom name) but the default is "Problem #"
\stepcounter{homeworkProblemCounter} % Increase counter for number of problems
\renewcommand{\homeworkProblemName}{#1} % Assign \homeworkProblemName the name of the problem
\section{\homeworkProblemName} % Make a section in the document with the custom problem count
\enterProblemHeader{\homeworkProblemName} % Header and footer within the environment
}{
\exitProblemHeader{\homeworkProblemName} % Header and footer after the environment
}

\newcommand{\problemAnswer}[1]{ % Defines the problem answer command with the content as the only argument
\noindent\framebox[\columnwidth][c]{\begin{minipage}{0.98\columnwidth}#1\end{minipage}} % Makes the box around the problem answer and puts the content inside
}

\newcommand{\homeworkSectionName}{}
\newenvironment{homeworkSection}[1]{ % New environment for sections within homework problems, takes 1 argument - the name of the section
\renewcommand{\homeworkSectionName}{#1} % Assign \homeworkSectionName to the name of the section from the environment argument
\subsection{\homeworkSectionName} % Make a subsection with the custom name of the subsection
\enterProblemHeader{\homeworkProblemName\ [\homeworkSectionName]} % Header and footer within the environment
}{
\enterProblemHeader{\homeworkProblemName} % Header and footer after the environment
}
   
%----------------------------------------------------------------------------------------
%       NAME AND CLASS SECTION
%----------------------------------------------------------------------------------------

\newcommand{\hmwkTitle}{Assignment\ \#1 } % Assignment title
\newcommand{\hmwkDueDate}{Tuesday,\ September\ 23,\ 2015} % Due date
\newcommand{\hmwkClass}{CSCI-567} % Course/class
\newcommand{\hmwkClassTime}{} % Class/lecture time
\newcommand{\hmwkAuthorName}{Saket Choudhary} % Your name
\newcommand{\hmwkAuthorID}{2170058637} % Teacher/lecturer
%----------------------------------------------------------------------------------------
%       TITLE PAGE
%----------------------------------------------------------------------------------------

\title{
\vspace{2in}
\textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
\normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
\vspace{0.1in}\large{\textit{\hmwkClassTime}}
\vspace{3in}
}

\author{\textbf{\hmwkAuthorName} \\
        \textbf{\hmwkAuthorID}
        }
\date{} % Insert date here if you want it to appear below your name

%----------------------------------------------------------------------------------------
\tikzstyle{level 1}=[level distance=1.5cm, sibling distance=2.5cm]
\tikzstyle{level 2}=[level distance=1cm, sibling distance=2cm]
\tikzstyle{level 3}=[level distance=1cm, sibling distance=1.5cm]

\begin{document}

\maketitle

%----------------------------------------------------------------------------------------
%       TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

%\setcounter{tocdepth}{1} % Uncomment this line if you don't want subsections listed in the ToC

\newpage
\tableofcontents
\newpage


%----------------------------------------------------------------------------------------
%       PROBLEM 2
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}[Problem 1] % Custom section title


\begin{homeworkSection}{\homeworkProblemName: ~(a) 1} % Section within problem
%\lipsum[4]\vspace{10pt} % Question

\problemAnswer{ % Answer
Given: $X_i \sim Beta(\alpha, 1)$
MLE for $\alpha$:

Consider $X=(X_1,X_2, \dots, X_n)$
Likelihood function: $L(\alpha|X)$
$L(\alpha | X) = \prod_{i=1}^n f(x_i)$
where
\begin{align*}
 f(x_i)&=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1} = \frac{\Gamma(\alpha +1)}{\Gamma(\alpha)\Gamma(1)}x^{\alpha-1}\\
 &=\frac{\alpha \Gamma(\alpha)}{\Gamma(\alpha)} x^{\alpha-1}\\
 &=\alpha x^{\alpha-1}	
\end{align*}

\begin{align*}
L(\alpha|X)&=\big(\frac{\Gamma(\alpha+1)}{\Gamma(\alpha)\Gamma(1)}\big)^n \prod_{i=1}^n (x_i)^{\alpha-1}
\end{align*}

\begin{align*}
LL=\log(L(\alpha | X)) &= n \log(\alpha) + (\alpha -1)\sum_{i=1}^n x_i \\
\frac{dLL}{d\alpha} &= \frac{n}{\alpha} + \sum_{i=1}^n{\log(x_i)}\\
\frac{dLL}{d\alpha} &= 0 \implies \hat{\alpha} =  \frac{n}{\sum_{i=1}^n log(1/x_i)}
\end{align*}
Minima at $\hat{\alpha} =  \frac{n}{\sum_{i=1}^n log(1/x_i)}$ is guaranteed due to $\log$ being a concave function.

Thus, 
\centering
$\boxed{\hat{\alpha_{MLE}} =  \frac{n}{\sum_{i=1}^n log(1/x_i)}}$

}




\end{homeworkSection}

%--------------------------------------------

\begin{homeworkSection}{\homeworkProblemName: ~(a) 2} % Section within problem
\problemAnswer{ % Answer
Given: $x_i \sim N(\theta, \theta)$ i.e $f(x_i)= (2\pi \theta)^{-\frac{1}{2}} e^{-\frac{(x_i-\theta)^2}{2\theta}}$
MLE estimate for $\theta$:

\begin{align*}
L(\theta|X) &=  (2\pi \theta)^{-\frac{N}{2}} e^{-\sum_{i=1}^n\frac{(x_i-\theta)^2}{2\theta}}\\
LL=\log(L(\theta|X)) &= -\frac{N}{2} \log((2\pi \theta)) -\sum_{i=1}^n\frac{(x_i-\theta)^2}{2\theta} \\
\frac{dLL}{d\theta} &= -\frac{N}{2}(\frac{1}{\theta})+\frac{\sum_{i=1}^n x_i^2}{2\theta^2}-\frac{N\theta}{2}\\
\frac{dLL}{d\theta} &=0 \implies N\theta^2+N\theta-\sum_{i=1}^n x_i^2 =0
\end{align*}
The above equation is a quadratic and will have two solutions,
Since, $\theta \geq 0$ (a constraint that comes from $\theta$ being the variance), the 

\begin{align*}
\theta = \frac{-N\pm \sqrt{N^2+4N\sum_{i=1}^nx_i^2}}{2N}
\end{align*}


Since, $\hat{\theta} \geq 0$ $\implies$
\centering
 $\boxed{\hat{\theta_{MLE}} = \frac{-N + \sqrt{N^2+4N\sum_{i=1}^nx_i^2}}{2N}}$

}
\end{homeworkSection}

%--------------------------------------------


\begin{homeworkSection}{\homeworkProblemName: ~(b) 1} % Using the problem name elsewhere
\problemAnswer{ % Answer
Given: $\hat{f(x)}=\frac{1}{n}\sum_{i=1}^n \frac{1}{h}K(\frac{x-X_i}{h})$
To show: $E_{X_1,X_2, \dots X_n}[\hat{f(x)}]=\frac{1}{h}\int K(\frac{x-t}{h})f(t)dt $

Proof:\\
\begin{align*}
E[\hat{f(x)}] &= E[\frac{1}{n}\sum_{i=1}^n \frac{1}{h}K(\frac{x-X_i}{h})]\\
&=\frac{1}{nh}E[K(\frac{x-X_i}{h})]\\
&=\frac{1}{h}E[K(\frac{x-X_1}{h})]\\
&=\frac{1}{h}E[K(\frac{x-t}{h})]
\end{align*}

where the penultimate equality comes from the fact that $X_i$ are iid for all $i \in [1,n]$.
and $t ~ X$%E_{X_1,X_2, \dots X_n}[\hat{f(x)}] = \int \hat{f(x)}f(x)dx

and hence.
\begin{align*}
E[\hat{f(x)}]&=\frac{1}{h}E[K(\frac{x-X_1}{h})]\\
&=\frac{1}{h}\int K(\frac{x-t}{h}) f(t)dt\\
& = RHS  
\end{align*}

}
\end{homeworkSection}

%--------------------------------------------

\begin{homeworkSection}{\homeworkProblemName: ~(b) 2}
\problemAnswer{ % Answer
Consider $z = \frac{x-t}{h}$ $\implies$ $t=x-hu$

Then, \begin{align*}
E[\hat{f(x)}] &= \frac{1}{h} \int K(z)f(x-hz)dz\\
f(x-hz) &= f(x) - f'(x)hz + \frac{1}{2}f''(x)\frac{(hz)^2}{2} - \frac{1}{3}f'''(x)\frac{(hz)^3}{3!} + \dots + (-1)^{n}\frac{1}{n!}f^{(n)}(x)(\frac{(hu)^n}{n!})
 \end{align*}

By definition, $\int k(z)dz =1$.

Also define an auxillary variable $M_j=\int k(z)z^j dz$ for the $j^{th}$ moment of the kernel function, 
and hence,

\begin{align*}
\int  K(z)f(x-hz)dz=f(x)-hf'(x)M_1+\frac{1}{2}(h^2)f^{''}(x)M_2+\dots+(-1)^n\frac{1}{n!}f^{(n)}M_n
\end{align*}

Now, 

\begin{align*}
Bias = E[\hat{f(x)}]-f(x)=-hf'(x)M_1+\frac{1}{2}(h^2)f^{''}(x)M_2+\dots+(-1)^n\frac{1}{n!}f^{(n)}M_n
\end{align*}



\centering $M_1 = \int xK(x)dx =0$

Hence,
\begin{align*}
\boxed{E[\hat{f(x)}]-f(x)=\frac{1}{2}(h^2)f^{''}(x)M_2+\dots+(-1)^n\frac{1}{n!}f^{(n)}M_n\ \ \text{where $M_j=\int k(z)z^j dz$}}
\end{align*}


And as $h \longrightarrow 0$, $Bias \longrightarrow 0$
}
\end{homeworkSection}

%--------------------------------------------

\end{homeworkProblem}

%----------------------------------------------------------------------------------------
%       PROBLEM 4
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}[Problem 2] % Roman numerals

\begin{homeworkSection}{\homeworkProblemName: ~(a)}
\problemAnswer{

	
		Mean $\bar{x} = \frac{1}{N}\sum x = \frac{1}{10}\sum_{i=1}^{10}x_i = 8.6$
		
		Mean $\bar{y} = \frac{1}{N}\sum y = \frac{1}{10}\sum_{i=1}^{10}y_i = 19.6$
		
		Standard deviation $x_{sd} = \sqrt{\frac{\sum_{i=1}^{10} (x_i-\bar{x})^2}{10-1}} = 21.3269$ 
		
		Standard deviation $y_{sd} = \sqrt{\frac{\sum_{i=1}^{10} (x_i-\bar{x})^2}{10-1}} = 25.1960$ 
		
		Student with unknown major: $(9,18)$: 
		
		Normalised to : $(0.0187, -0.0635)$ 
		
\begin{center}


		 \begin{tabular}{|c|c|c|c|c|c|c|}
			\hline \rule[-2ex]{0pt}{5.5ex} ID & x  & y & $x_n$ & $y_n$ & L1 & L2\\ 
			\hline \rule[-2ex]{0pt}{5.5ex} M1 & 10 & 49 & 0.0623 & 1.107  & \underline{1.2117} & 1.168\\ 
			\hline \rule[-2ex]{0pt}{5.5ex} M2 & -12 & 38 & -0.9163 & 0.6928  & 1.6871 & 1.1998 \\ 
			\hline \rule[-2ex]{0pt}{5.5ex} M3 & -9 & 47 & -0.7829 & 1.0317 & 1.8926 & 1.354 \\ 
			\hline \rule[-2ex]{0pt}{5.5ex} EE1 & 29 & 19 & 0.9074 & -0.0226 & \underline{0.9272} & \underline{0.8904}\\ 
			\hline \rule[-2ex]{0pt}{5.5ex} EE2 & 32 & 31 & 1.0409 & 0.4292 & 1.5125 & \underline{1.1341}\\ 
			\hline \rule[-2ex]{0pt}{5.5ex} EE3 & 37 & 38 & 1.2633 & 0.6928 & 1.9985 & 1.4554\\
			\hline \rule[-2ex]{0pt}{5.5ex} CS1 & 8 & 9 & -0.0267 & -0.3991 & \underline{0.3834} & \underline{0.3418} \\ 
			\hline \rule[-2ex]{0pt}{5.5ex} CS2 & 30 & -28 & 0.9519 & -1.7922 & 2.6661 & 1.9678 \\ 
			\hline \rule[-2ex]{0pt}{5.5ex} CS3 & -18 & -19 & -1.1832 & -1.4534 & 2.5942 & 1.8394 \\ 
			\hline \rule[-2ex]{0pt}{5.5ex} CS4 & -21 & 12 & -1.3167 & -0.2862 & 1.5605 & 1.3535 \\ 
			\hline 
		\end{tabular} 
\end{center}		
	Procedure: We first normalise the data point with unknown major using the mean and standard deviation of the known points, and then calculated the L1 and L2 distances.
	L1 distance between two points $(x_1,y_1)$ and $(x_0,y_0$) is defined as : $L1 = |x_1-x_0| +  |y_1-y_0|$
	
	L2 distance is defined as $L2 = \sqrt{(x_1-x_0)^2 + (y_1-y_0)^2}$
	
	
		\textbf{For $L1$}:
		
		
		\textbf{$K=1$}: For $K=1$ the nearest neighbor is CS1 and hence the unknown sample 'could' be a computer science
		
		\textbf{$K=3$}: For $K=3$ the nearest neighbors are M1, EE1, CS1 and hence there is a 'tie'. Choosing the label of the least distance would again result in CS1 as $CS1 < EE1 <M1$.
		
		\textbf{For $L2$}:
		
		
		\textbf{$K=1$}: For $K=1$ the nearest neighbor is CS1 and hence the unknown sample 'could' be a computer science
		
		\textbf{$K=3$}: For $K=3$ the nearest neighbors are M1, EE1, EE2. Since two nearest neighbors are from EE, we assign it the unknown sample to be from Electrical engineering.
	}
	\clearpage
	\problemAnswer{
		
		\textbf{Comparison} For $K=1$, both $L1$ and $L2$ distance metric give the same results, however for $K=3$, the $L1$ metric yields a tie, since the distances are similar but $L2$ metric being a square quantity of a number smaller than 1 further reduces the distances. The fact to realise is that $|x+y|$ is similar to $\sqrt{x^2+y^2}$ when $x,y << 1$ that is when the points are close, but when $x,y$ are large, the L2 metric is going to be higher, and hence L2 norm applies more 'penalty' to distant points in the sense that they are larger. 
		
		\textit{ This also implies that in case of outliers, while $L1$ norm will not penalise, $L2$ norm will penalise in the sense that $\sqrt{x^2+y^2}$ will be high.
			In case of outliers $L2$ is more robust. In this example, 'M1' is likely an outlier. }
		
		
		
		 
		

			
		
			

		
	}
\end{homeworkSection}


\begin{homeworkSection}{\homeworkProblemName: ~(b)}
	\problemAnswer{
		Total points: $N$
		
		Total points with label class $c$: $N_c$
		
		Given: $p(x|Y=c)=\frac{K_c}{N_cV}$ and $\sum K_c=K$
		Class prior: $p(Y=c)=\frac{N_c}{N}$
		
		Unconditional density $p(x)=\sum_c p(x|Y=c)p(Y=c) = \sum_c \frac{K_c}{N_cV} \times \frac{N_c}{N}= \sum_c \frac{K_c}{NV} = \frac{K}{NV}$
		
		Posterior $\boxed{P(Y=c|x)=\frac{P(x|Y=c)\times P(Y=c)}{ P(x) } = \frac{\frac{K_c}{N_cV} \times \frac{N_c}{N} }{\frac{K}{NV}} =\frac{K_c}{K} }$
		
		
	}
\end{homeworkSection}

\end{homeworkProblem}


\begin{homeworkProblem}[Problem 3] % Roman numerals
	
	\begin{homeworkSection}{\homeworkProblemName: ~(a)}
		%\problemAnswer{
			Information gain $G=H[Y]-H[Y|X]$ where $Y$ is the outcome variable and $X$ is an attribute to be split. 
			In our case $Y$= 'Rains or not' In order to maximise gain for a fixed $Y$ we need to minimise the conditional entropy $H[Y|X]$
			$p_{rain}=\frac{9+5+6+3+7+2+3+1}{80}=frac{36}{80}=0.9$ and hence $p_{no-rain}=0.1$
			\begin{align*}
					H[Y]&=-p_{rain}\log(p_rain)-p_{no-rain}\log(p_{no-rain})\\
						&= -(0.9\log_2(0.9)+0.1\log_2(0.1)\\
						&= 0.99277
			\end{align*}
			
			Instead of maximising gains, it is sufficient to simply minimise the conditional entropy $H[Y|X]$ in this case.
			
			\textbf{For Temperature}
			
			\begin{align*}
			\text{H[Rainy or Not rainy | Temperature]} &= -p_{hot}\times (p_{rain,hot}\log(p_{rain,hot}) + p_{norain, hot}\log(p_{norain,hot})\\ 
			&- p_{cold} \times (p_{rain,cold}\log(p_{rain,cold}) + p_{norain,cold}\log(p_{norain,cold}))\\
			&= -\frac{1}{2} \times (\frac{23}{40}\log(\frac{23}{40}) + \frac{13}{40}\log(\frac{13}{40}) )\\
			&-\frac{1}{2} \times (\frac{13}{40}\log(\frac{13}{40}) + \frac{27}{40}\log(\frac{27}{40}))\\
			& = 0.9467
			\end{align*}

			Similarly,
			
			\begin{align*}
			\text{H[Rain or Not rainy | Sky]} &= \frac{1}{2}\text{H[Rainy or Not rainy | Sky=Cloudy]}\\
			& + \frac{1}{2}\text{H[Rainy or Not rainy | Sky=Clear]}\\
			&= 0.9015
			\end{align*}
			
			Similarly,
			
			\begin{align*}
			\text{H[Rain or Not rainy | Humidity]} &= \frac{1}{2}\text{H[Rainy or Not rainy | Humidity=High]}\\
			 &+ \frac{1}{2}\text{H[Rainy or Not rainy | Humidity=Low]}\\
			&= 0.9467
			\end{align*}

			We choose $Sky$ as the *root* because $H[Rain or Not rainy | Sky]$ is minimum.
			
			Further on,
			\begin{align*}
			\text{H[Rainy or not|Cloudy,Temperature]} &= -\frac{1}{2}(\frac{15}{20}\log(\frac{15}{20})+\frac{5}{20}\log(\frac{5}{20}))\\
			&			 -\frac{1}{2}(\frac{10}{20}\log(\frac{10}{20})+\frac{10}{20}\log(\frac{10}{20}))\\
			&=0.9056
			\end{align*}
			
			
			\begin{align*}
			\text{H[Rainy or not|Cloudy,Humidity]} &= -\frac{1}{2}(\frac{16}{20}\log(\frac{16}{20})+\frac{4}{20}\log(\frac{4}{20}))\\
			&			 -\frac{1}{2}(\frac{9}{20}\log(\frac{9}{20})+\frac{11}{20}\log(\frac{11}{20}))\\
			&=0.8574
			\end{align*}
		
		Thus, we choose condition Humidity for Cloudy Sky.
		
		Further more, 
		\begin{align*}
		\text{H[Rainy or not|Clear,Temperature]} &= 0.7903\\
		\text{H[Rainy or not|Clear,Humidity]} &= 0.8280\\
		\end{align*}
		
		So we choose $Temperature$ condition for Clear Sky.
		Now for $Temperature=Hot, Sky=Clear$ we have $p_{rain}=\frac{8}{20}$, and $p_{norain}=\frac{12}{20}$. So we choose 'Not Rainy' for $Temperature=Hot$ branch.
		
		For, $Temperature=Cold, Sky=Clear$ we have $p_{rain}=\frac{3}{20}$ and hence once again we choose 'Not Rainy' for $Temperature=Cold$ branch.
		
		For $Humidity=High,Sky=Cloudy$, $p_{rain}=\frac{16}{20}$ and hence we choose $Rain$ for $Humidity=High,Sky=Cloudy$
		
		And, finally for $Humidity=Low,Sky=Cloudy$ , $p_{rain}=\frac{9}{20}$ and hence we choose $Not\ Rainy$ .
		
		The decision and the pruned decision tree is shown in Figure 1,2.
		
			
	%	}

	\begin{figure}
		\includegraphics[scale=0.5]{dt}
		\caption{Problem 3 Decision Tree}
	\end{figure}
	
	\begin{figure}
		\includegraphics[scale=0.5]{dt2}
		\caption{Problem 3 Pruned Decision Tree}
	\end{figure}
	
	
	
	\end{homeworkSection}
		\begin{homeworkSection}{\homeworkProblemName: ~(b)}
			\problemAnswer{
				Consider $f(p_k)=(1-p_k)-(- \log p_k)$ 
				We know that $0 \leq p_k \leq 1$
				Then $f'(p_k) = -1+\frac{1}{p_k} = -\frac{1-p_k}{p_k} \leq 0 \forall p_k \in[0,1]$
				
				And hence $f'(p_k)$ is a non-increasing function which $\implies$ $f(p_k) \geq f(1) \forall p_k \in (0,1]$
				and hence, $(1-p_k)-(- \log p_k) \geq 0$ $\implies$ $p_k(1-p_k)-(-p_k \log p_k) \geq 0$ $\implies$
				$p_k(1-p_k) \geq -p_k\log p_k$  $\implies$  $\sum_{k=1}^K p_k(1-p_k) \geq \sum_{k=1}^K-p_k\log p_k$ $\implies$ Gini index is less than corresponding value of Cross Entropy
				
			}
			
		\end{homeworkSection}
	
	\begin{homeworkSection}{\homeworkProblemName: ~(c)}
		\problemAnswer{
			By default the rightmost branch corresponds to the parent condition being a YES[FIX ME]
			
			
			\textbf{a}:
			\begin{center}

						\includegraphics{hw1a1}
					\begin{tikzpicture}
					\tikzstyle{every node}=[draw,rectangle]
					
					\node {$X1>x$}
					[style=edge from parent fork down]

					child {
						node {$X2>y$}
						child { node {$\circ$} }
						child { node {$\Delta$} }
					}
					child { node {$\Delta$} }
					;
					
					\end{tikzpicture}
			\end{center}
					
					
			\textbf{b}:
			\begin{center}
				\includegraphics{hw1b1}
				
				\begin{tikzpicture}
				\tikzstyle{every node}=[draw,rectangle]
				
				\node {$X1>x$}
				[style=edge from parent fork down]
				
				child {
					node {$X2>y$}
					child { node {$\circ$} }
					child { node {$\Delta$} }
				}
				child { 
					node {$X2>y$}
					child { node {$\Delta$} }
					child { node {$\circ$} }
				 }
				;
				
				\end{tikzpicture}
			\end{center}
			
			
				\textbf{c}:
				\begin{center}
					\includegraphics{hw1c1}
					The only case where it is not possible to have a depth 6 decision tree is case (c). The decision boundary in this case is a 'zig-zag' ladder and hence the depth of decision tree is unbounded.
					
%					\begin{tikzpicture}
%					\tikzstyle{every node}=[draw,rectangle]
%					
%					\node {$X1>x$}
%					[style=edge from parent fork down]
%					
%					child {
%						node {$\circ$}
%					}
%					child { 
%						node {$X2>y$}
%						child { node {$\circ$} }
%						child { node {$\Delta$} }
%					}
%					;
%					
%					\end{tikzpicture}
				\end{center}
				
			}
			\clearpage
			\problemAnswer{
				
				
					\textbf{d}:
					\begin{center}
						
						\includegraphics{hw1d1}
						\begin{tikzpicture}
						\tikzstyle{every node}=[draw,rectangle]
						
						\node {$X1>x2$}
						[style=edge from parent fork down]
						
						child { 
							node {$X1>x1$}
							child { node {$\Delta$} }
							child { 
								node {$X2>y2$} 
								child {
									child{node {$\Delta$}}
									child{node {$Y>y1$}
										child{
											child { node {$\circ$} }
											child { node {$\Delta$} }
										}
									}
								}
							}
						}
						child {
							node {$\Delta$}
						}
						;
						
						\end{tikzpicture}
					\end{center}
			
			
			
			}

	\end{homeworkSection}
\end{homeworkProblem}

\begin{homeworkProblem}[Problem 4]
	\begin{homeworkSection}{\homeworkProblemName: ~(a)}
			\problemAnswer{
				 Given a random variable $X \in \mathbf{R}^D$ and $Y \in [C]$ naive bayes defines the joint distribution:
				 \begin{align*}
					P(X=x,Y=y)&=P(Y=y)P(X=x|Y=y)\\
					&=P(Y=y)\prod_{d=1}^D P(X_d=x_d|Y=y)
				 \end{align*}
				 
					Y is a categorical variable with $P(Y=k)=p_k$ for $k \in [1,K]$
					
					Given: $P (x_j |Y=y_k )\sim N(\mu_{jk}, \sigma_{jk})$ $\implies$
					
					\begin{equation}
						\log(P(x_j|Y=k)) = -\frac{\log(2\pi \sigma_{jk})}{2} - \frac{(x_j-x_{jk})^2}{2\sigma_{jk}}
					\end{equation}
					
					

					
					For all $j\neq j'$ $x_j,X_{j'}$ are independent attributes.
					
					Naive bayes: 
					\begin{align*}
						P(X_i=\vec{x},Y_i=y)&=P(Y_i=y)P(X_{i1}=x_1,X_{i2}=x_2\dots X_{iD}=x_D|Y=y_k)\\
						&=P(Y_i=y) \prod_{j=1}^D P(X_{ij}=x_j|Y=y_i)\ \text{Assuming independence of attributes $x_i$}
					\end{align*}
					Each $Y_i$ belongs to one of the $K$ classes, thus $\sum_{k=1}^K p_k=1$ for any $Y_i$
					
					Let $N_k$ represent the number of elements in class $k$ for $k \in [1,K]$
					
					
					Then, $\sum_{i=1}^N \log(Y_i=y_i) = \sum_{k=1}^K P(Y=k) \times N_k $
					
					Consider the likelihood function 
					\begin{align*}
						L(\mu,\sigma,p|(X,Y)) &=  \prod_{1}^N P(Y_i=y_i)\times \prod_{j=1}^D P(X_i=x_{ij}|Y=y_i)\\
						\log(L)&= \sum_{i=1}^N \log(P(Y_i=y_i)) +\sum_{i=1}^N \sum_{j=1}^D \log(P(X_i=x_{ij}|Y=y_{i}))\\
						&= \sum_{i=1}^N \log(P(Y_i=y_i)) +\sum_{i=1}^N \sum_{j=1}^D \log(P(X_{ij}=x_{ij}|Y=y_{i}))\\
						&=\sum_{k=1}^K P(Y=k) \times N_k + \sum_{k=1}^K \sum_{j=1}^D \log(P(X_{ij}=x_{ij}|Y=k)) \times N_k
					\end{align*}
					
					Now, 
					\begin{align}
					\label{rho1}\frac{\partial{LL}}{\partial{p_k}}&=0\\
					\label{rho2}\frac{\partial{LL}}{\partial{\mu_{jk}}}&=0\\
					\label{rho3}\frac{\partial{LL}}{\partial{\sigma{jk}}}&=0\\
					\end{align}
				}
				\clearpage
				\problemAnswer{
					For for equation \ref{rho1} and constraint $\sum_k p_k =1$, we get: $p_k \frac{N_k}{N}$
					\clearpage
					For equation \ref{rho2},
					\begin{align*}
						\frac{\partial{\sum_{k=1}^K \sum_{j=1}^D \log(P(X_i=x_{ij}|Y=k)) \times N_k}}{\partial \mu_{jk}} &=0\\
						\frac{\sum_{i;Y_i=k}(x_{ij}-\mu_{jk})}{\sigma_{jk}}=0\\
						%-\frac{\sum_{k=1}^KN_k(x_j-\mu_{jk})}{\sigma_{jk}} &=0\\
						\hat{\mu_{jk}} = \frac{\sum_{i;Y_i=k}x_{ij}}{N_k}
					\end{align*}
					
					For equation \ref{rho3},
					\begin{align*}
						\frac{\partial{\sum_{k=1}^K \sum_{j=1}^D \log(P(X_i=x_{ij}|Y=k)) \times N_k}}{\partial \sigma_{jk}} &=0\\
						\frac{\partial}{\partial{\sigma_{jk}}}\sum_{i;Y_i=k}\big(-\frac{\log(2\pi \sigma_{jk})}{2} - \frac{(x_{ij}-x_{jk})^2}{2\sigma_{jk}}\big)=0\\
						\frac{\partial}{\partial{\sigma_{jk}}}\sum_{i;Y_i=k}\big(-\frac{1}{\sigma_{jk}} + \frac{(x_{ij}-x_{jk})^2}{2\sigma_{jk}^2}\big)=0\\
						\hat{\sigma_{jk}}=\frac{\sum_{i;Y_i=k}(x_{ij}-\hat{\mu_{jk}})^2}{N_k}
					\end{align*}
					Constraint $\boxed{\sum_k p_k =1}$
					Given $K$ number of classes, the above constraint the MLE estimate of $p_k$ is given by: $\boxed{\frac{N_k}{N}}$
				}
	\end{homeworkSection}
		\begin{homeworkSection}{\homeworkProblemName: ~(b)}
			\problemAnswer{
				Given: $P(Y=1)=\pi$; For $X_j$ feature, $P(X_j=x_j|Y_k)=\theta_{jk}^{x_j} (1-\theta_{jk})^{1-x_j}$
			\begin{align*}
				P(Y=1|X) &= \frac{P(X|Y=1)P(Y=1)}{P(X)}\\
				 &= \frac{P(X|Y=1)P(Y=1)}{P(X|Y=1)P(Y=1)+P(X|Y=0)P(Y=0)}\\
				 &= \frac{1}{1+\frac{P(X|Y=0)P(Y=0)}{P(X|Y=1)P(Y=1)}}\\
				 &= \frac{1}{1+\exp(\log(\frac{P(X|Y=0)P(Y=0)}{P(X|Y=1)P(Y=1)}))}\\
				 &= \frac{1}{1+\exp(\log({P(X|Y=0)P(Y=0)})-\log({P(X|Y=1)P(Y=1)}))}\\
 				 &= \frac{1}{1+\exp(-(\log(\frac{P(Y=1)}{P(Y=0)}))+\log(P(X|Y=0))-\log(P(X|Y=1)))}\\
			\end{align*}
			
			Now assuming features satisfy the independence property, $P(X|Y=1) = \prod_{j=1}^D P(X_j|Y=1) = \prod_{j=1}^D \theta_{jk}^{x_j} (1-\theta_{jk})^{1-x_j}$
			
			Alternatively,
			
			\begin{align}
			\label{log4b}
				\log(P(X_j|Y=1)) &= \log(\theta_{j1}^{x_j} (1-\theta_{j1})^{1-x_j})\\ 
				&= x_j \log(\theta_{j1}) + (1-x_j)\log((1-\theta_{j1}) \\
				&= x_j  \log(\frac{\theta_{j1}}{1-\theta_{j1}}) + \log(1-\theta_{j1})	
			\end{align}
			
			and,
			
			\begin{align}
		\label{log4c}
		\log(P(X_j|Y=0)) &= \log(\theta_{j0}^{x_j} (1-\theta_{j0})^{1-x_j})\\ 
		&= x_j \log(\theta_{j0}) + (1-x_j)\log((1-\theta_{j0}) \\
		&= x_j  \log(\frac{\theta_{j0}}{1-\theta_{j0}}) + \log(1-\theta_{j0})	
		\end{align}
			
			Hence,
			
			\begin{equation}
			\label{log4d}
			\log(P(X_j|Y=0) - \log(P(X_j|Y=1)) = x_j\log(\frac{\theta_{j0}(1-\theta_{j1})}{\theta_{j1}(1-\theta_{j0})}) + \log\frac{(1-\theta_{j0})}{(1-\theta_{j1})}
			\end{equation}
			$\implies$
			
			\begin{equation}
			\log(P(X|Y=0) - \log(P(X|Y=1)) = \sum_{j=1}^D x_j\log(\frac{\theta_{j0}(1-\theta_{j1})}{\theta_{j1}(1-\theta_{j0})}) + \sum_{j=1}^D\log\frac{(1-\theta_{j0})}{(1-\theta_{j1})}
			\end{equation}
		}			\newpage
		\problemAnswer{

			\begin{align*}
			-(\log(\frac{P(Y=1)}{P(Y=0)})) + \log(P(X|Y=0) - \log(P(X|Y=1)) &= \sum_{j=1}^D x_j\log(\frac{\theta_{j0}(1-\theta_{j1})}{\theta_{j1}(1-\theta_{j0})}) +\\ &+ \big(\log(\frac{(1-\theta_{j0})}{(1-\theta_{j1})}+-(\log(\frac{P(Y=1)}{P(Y=0)}))\big)\\
			&= \sum_{j=1}^D x_j\log(\frac{\theta_{j0}(1-\theta_{j1})}{\theta_{j1}(1-\theta_{j0})}) +\\ &+
			\big(\log(\frac{(1-\theta_{j0})}{(1-\theta_{j1})}+(\log(\frac{P(Y=0)}{P(Y=1)}))\big)\\
			&= \sum_{j=1}^D x_j\log(\frac{\theta_{j0}(1-\theta_{j1})}{\theta_{j1}(1-\theta_{j0})}) +\\ &+
			\big(\log(\frac{(1-\theta_{j0})}{(1-\theta_{j1})}+(\log(\frac{(1-\pi)}{\pi}))\big)			
			\end{align*}
			And hence 
			\centering
			$\boxed{\vec{w_j} = \log(\frac{\theta_{j0}(1-\theta_{j1})}{\theta_{j1}(1-\theta_{j0})})}$ 	
			
			$\boxed{w_0= -\log(\frac{1-\pi}{\pi} \times (\frac{1-\theta_{j0}}{1-\theta_{j1}})^D)}$
		}
		
		\end{homeworkSection}	
\end{homeworkProblem}
\clearpage

\begin{homeworkProblem}[Problem 5]
	\begin{homeworkSection}{\homeworkProblemName: ~(5.1)}
		%\problemAnswer{
		
		\begin{tabular}{ll}
			\includegraphics[scale=0.45]{KDE_h_01} & \includegraphics[scale=0.45]{KDE_h05}\\
			\includegraphics[scale=0.45]{KDE_h7} & \includegraphics[scale=0.45]{KDE_h09}\\
				\includegraphics[scale=0.45]{KDE_h_1} & \includegraphics[scale=0.45]{KDE_h_5}\\
		\end{tabular}
		%	}
		
		\begin{figure}
			\includegraphics[scale=0.7]{variance}
			\label{fig:var}
			\caption{Integrated Variance distribution with respect to h for various kernels}
		\end{figure}
		
		
		
		\problemAnswer{
			From figure 3\ref{fig:var} we see that Gaussian kernel guarantees the minal variance for all values of $h$. Histogram consistently leads to higher variance and Epanechnikov kernels' variance is bounded between the former two. And hence we conclude Gaussian kernel outperforms Epanechnikov kernel which outperforms histogram for kernel density estimation(based on the criteria of minimising the variance)
			
			An optimum value of $h$ is found at the $knee$ of the graph and is approximately $h=0.1$. Selecting the $knee$ guarantees minimal varaince and an optimum choice for a smaller $h$, because higher $h$ are guaranteed to minimise the variance.
				
			}
	\end{homeworkSection}	
	
	\begin{homeworkSection}{\homeworkProblemName: ~(5.2d)}
		\problemAnswer{
			\begin{center}
				%\begin{table}
				Accurcy table for k-nn\\
			\begin{tabular}{|c|c|c|c|}
				\hline \rule[-2ex]{0pt}{5.5ex}  K &  Training Accuracy & Validation Accuracy & Testing Accuracy
				\\ 
				\hline \rule[-2ex]{0pt}{5.5ex}  K=1 &  0.788973 & 0.759259 & 0.776744
				  \\ 
				\hline \rule[-2ex]{0pt}{5.5ex}  K=3 &  0.768061 & 0.717593 & 0.744186  \\ 
				\hline \rule[-2ex]{0pt}{5.5ex}  K=5&  0.798479 & 0.782407 & 0.753488  \\ 
				\hline \rule[-2ex]{0pt}{5.5ex}  K=7&  0.861217 & 0.842593 & 0.809302  \\ 
				\hline \rule[-2ex]{0pt}{5.5ex}  K=9&  0.897338 & 0.902778 & 0.865116 \\ 
				\hline \rule[-2ex]{0pt}{5.5ex}  K=11& 0.908745 & 0.907407 & 0.883721   \\ 
				\hline \rule[-2ex]{0pt}{5.5ex}  K=13& 0.866920 & 0.861111 & 0.823256  \\ 
				\hline \rule[-2ex]{0pt}{5.5ex}  K=15&  0.817490 & 0.782407 & 0.762791  \\ 
				\hline 
			\end{tabular} 
			\newline
			\newline
			\vspace{20pt}
			\textit{Based on the maximum training accuracy, we can choose K to be 11}
			%\caption{Accuracy for different values of K for k-nn}
			%\end{table}
			\newline
		\footnotesize
		Gini Index = GDI\\
		Cross Entropy = DEV\\
		Accuracy table for Decision Tree\\
			\begin{tabular}{|l|l|l|l|l|l|l|}
				\hline  MinLeaf& Training(GDI) & Training(DEV)  & Validation(GDI) & Validation(DEV) & Testing(GDI)  & Testing(DEV)  \\ 
				\hline
				1 & 0.950664 & 0.950664 & 0.870370 & 0.842593 & 0.860465 
				& 0.865116\\ \hline
				2 & 0.950664 & 0.950664 & 0.870370 & 0.842593 & 0.860465 
				& 0.865116\\ \hline  
				3 & 0.950664 & 0.950664 & 0.870370 & 0.842593 & 0.860465 
				& 0.865116\\ \hline 
				4 & 0.948767 & 0.948767 & 0.875000 & 0.847222 & 0.883721 
				& 0.883721\\ \hline  
				5 & 0.941176 & 0.939279 & 0.875000 & 0.856481 & 0.874419 
				& 0.869767  \\ \hline  
				6 & 0.935484 & 0.933586 & 0.861111 & 0.842593 & 0.888372 
				& 0.883721 \\ \hline
				7 & 0.927894 & 0.925996 & 0.870370 & 0.851852 & 0.897674 
				& 0.893023\\ \hline
				8 & 0.920304 & 0.918406 & 0.856481 & 0.837963 & 0.897674 
				& 0.893023 \\ \hline
				9 & 0.914611 & 0.882353 & 0.879630 & 0.824074 & 0.860465 
				& 0.823256 \\ \hline
				10 & 0.912713 & 0.880455 & 0.856481 & 0.800926 & 0.888372 
				& 0.851163 \\ \hline
				\hline 
			\end{tabular} 
			
		\end{center}
					\vspace{20pt}
		\textbf{Naive Bayes Classifier}
\begin{tabular}{|c|c|c|c|}
	\hline Dataset & Training Accuracy & Validation Accuracy & Testing Accuracy \\ 
	\hline  Nursery & 0.9065 & 0.9059 & 0.8970\\ 
	\hline  TTT& 0.7827 & 0.7130 & 0.7302\\
	\hline 
\end{tabular}\\ 
\newline
\newline
%%\vspace*{20pt}
As evident from the results for Naive Bayes, it performs pretty good on Nursery dataset, since its attributes can be treated to be independent. On the other hand, tic tac toe dataset has an inherent dependence built in that decides the labels. For example, a XXX is a 'Positive' for sure and XXO can never be. This violates the inherent assumption of independence of features in Naive Bayes and hence the lower accuracy.
			
		
			}
	\end{homeworkSection}	
	
	\begin{homeworkSection}{\homeworkProblemName: ~(5.2e)}
	\begin{tabular}{ll}
		\includegraphics[scale=0.4]{boundary_k1} & 		\includegraphics[scale=0.4]{boundary_k5}\\
		\includegraphics[scale=0.4]{boundary_k15} &
						\includegraphics[scale=0.4]{boundary_k25}
				
		
	\end{tabular}
	\problemAnswer{
		As evident from the plots above, increasing $K$ results in more smooth boundaries. This is expected,because $k$ increasing leads to more neihbors being weighted for deciding the final label, and this will often involve neighbors that are far apart, thus creating the soft boundaries}
	\end{homeworkSection}	
	
\end{homeworkProblem}
\
%----------------------------------------------------------------------------------------

\end{document}
