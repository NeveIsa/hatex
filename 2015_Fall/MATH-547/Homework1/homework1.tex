%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Structured General Purpose Assignment
% LaTeX Template
%
% This template has been downloaded from:
% http://www.latextemplates.com
%
% Original author:
% Ted Pavlic (http://www.tedpavlic.com)
%
% Note:
% The \lipsum[#] commands throughout this template generate dummy text
% to fill the template out. These commands should all be removed when 
% writing assignment content.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%       PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage{graphicx} % Required to insert images
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template

\usepackage{amsmath}

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in 

\linespread{1.1} % Line spacing

% Set up the header and footer
\pagestyle{fancy}
\lhead{\hmwkAuthorName} % Top left header
\chead{\hmwkClass\ : \hmwkTitle} % Top center header
\rhead{\firstxmark} % Top right header
\lfoot{\lastxmark} % Bottom left footer
\cfoot{} % Bottom center footer
\rfoot{Page\ \thepage\ of\ \pageref{LastPage}} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule

\setlength\parindent{0pt} % Removes all indentation from paragraphs

%----------------------------------------------------------------------------------------
%       DOCUMENT STRUCTURE COMMANDS
%       Skip this unless you know what you're doing
%----------------------------------------------------------------------------------------

% Header and footer for when a page split occurs within a problem environment
\newcommand{\enterProblemHeader}[1]{
\nobreak\extramarks{#1}{#1 continued on next page\ldots}\nobreak
\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
}

% Header and footer for when a page split occurs between problem environments
\newcommand{\exitProblemHeader}[1]{
\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
\nobreak\extramarks{#1}{}\nobreak
}

\setcounter{secnumdepth}{0} % Removes default section numbers
\newcounter{homeworkProblemCounter} % Creates a counter to keep track of the number of problems

\newcommand{\homeworkProblemName}{}
\newenvironment{homeworkProblem}[1][Problem \arabic{homeworkProblemCounter}]{ % Makes a new environment called homeworkProblem which takes 1 argument (custom name) but the default is "Problem #"
\stepcounter{homeworkProblemCounter} % Increase counter for number of problems
\renewcommand{\homeworkProblemName}{#1} % Assign \homeworkProblemName the name of the problem
\section{\homeworkProblemName} % Make a section in the document with the custom problem count
\enterProblemHeader{\homeworkProblemName} % Header and footer within the environment
}{
\exitProblemHeader{\homeworkProblemName} % Header and footer after the environment
}

\newcommand{\problemAnswer}[1]{ % Defines the problem answer command with the content as the only argument
\noindent\framebox[\columnwidth][c]{\begin{minipage}{0.98\columnwidth}#1\end{minipage}} % Makes the box around the problem answer and puts the content inside
}

\newcommand{\homeworkSectionName}{}
\newenvironment{homeworkSection}[1]{ % New environment for sections within homework problems, takes 1 argument - the name of the section
\renewcommand{\homeworkSectionName}{#1} % Assign \homeworkSectionName to the name of the section from the environment argument
\subsection{\homeworkSectionName} % Make a subsection with the custom name of the subsection
\enterProblemHeader{\homeworkProblemName\ [\homeworkSectionName]} % Header and footer within the environment
}{
\enterProblemHeader{\homeworkProblemName} % Header and footer after the environment
}
   
%----------------------------------------------------------------------------------------
%       NAME AND CLASS SECTION
%----------------------------------------------------------------------------------------

\newcommand{\hmwkTitle}{Assignment\ \# 1} % Assignment title
\newcommand{\hmwkDueDate}{Monday,\ September\ 21,\ 2015} % Due date
\newcommand{\hmwkClass}{MATH-547} % Course/class
\newcommand{\hmwkClassTime}{} % Class/lecture time
\newcommand{\hmwkAuthorName}{Saket Choudhary} % Your name
\newcommand{\hmwkAuthorID}{2170058637} % Teacher/lecturer
%----------------------------------------------------------------------------------------
%       TITLE PAGE
%----------------------------------------------------------------------------------------

\title{
\vspace{2in}
\textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
\normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
\vspace{0.1in}\large{\textit{\hmwkClassTime}}
\vspace{3in}
}

\author{\textbf{\hmwkAuthorName} \\
        \textbf{\hmwkAuthorID}
        }
\date{} % Insert date here if you want it to appear below your name

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle

%----------------------------------------------------------------------------------------
%       TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

%\setcounter{tocdepth}{1} % Uncomment this line if you don't want subsections listed in the ToC

\newpage
\tableofcontents
\newpage


\begin{homeworkProblem}[Problem 1] % Roman numerals
%--------------------------------------------
		$(X,Y) \in {R}^d \times \{\pm 1\}$
		\begin{align*}
		P(Y=1) &= \pi_+\\
		P(Y=-1) &= \pi_-\\
		P(dx|Y=1) &= p_{+}(x)\\
		P(dx|Y=-1) &= p_{-}(x)
		\end{align*}
\begin{homeworkSection}{\homeworkProblemName: (a)} % Section within problem
	
\problemAnswer{ % Answer

Expression for bayes classifier:

\begin{align*}
P(Y=1|x)= \frac{P(x|Y=1)P(Y=1)}{P(x|Y=1)P(Y=1)+P(x|Y=-1)P(Y=-1)}\\
P(Y=1|x) = \frac{\pi_+ p_{+}(x)}{\pi_+ p_{+}(x)+\pi_-p_{-}(x)}
\end{align*}

\begin{align*}
P(Y=-1|x)= \frac{P(x|Y=-1)P(Y=1)}{P(x|Y=-1)P(Y=-1)+P(x|Y=-1)P(Y=-1)}\\
P(Y=-1|x) = \frac{\pi_- p_{-}(x)}{\pi_+ p_{+}(x)+\pi_-p_{-}(x)}
\end{align*}

Define $\eta(x) =  E[Y|x] = 1\times P(Y=1|x) + -1 \times P(Y=1|x) $

Now, 

%\begin{align*}
%P(error|x) = \begin{cases}
%P(Y=1|x) & \text{if actual class for x is -1}\\
%P(Y=-11|x) & \text{if actual class for x is 1}\\
%\end{cases}
%\end{align*}

Intuitively to minimise the error, we choose class '1' for $x$ when
$P(Y=1|x)>P(Y=-1|X)$

and hence, the bayes classifier $g_*(x)$ is given by:\\
\begin{align*}
g_*(x) = \begin{cases}
1 & \pi_+p_+(x) \geq \pi_{-}p_-(x)\\
-1 & \text{otherwise}
\end{cases}
\end{align*}

Where, $ \pi_+p_+(x) \geq \pi_{-}p_-(x)$ is given by:

\begin{align*}
\log(\pi_+p_+(x))  \geq \log(\pi_{-}p_-(x))\\
\log(\pi_+) + \log(p_+(x))  \geq \log(\pi_{-}) + 
log(p_-(x))\\
\log(\pi_+)  + \sqrt{\frac{1}{2\pi}} \mathbf{\sigma}^{-1} (x-\vec{a_+})\mathbf{\sigma}^{-1}(x-\vec{a_+})^T \geq \log(\pi_-) + \sqrt{\frac{1}{2\pi}} \mathbf{\sum}^{-1} (x-\vec{a_-})\mathbf{\sum}^{-1}(x-\vec{a_-})^T
\end{align*}


}

%and the total probability of making an error is given by:
%$P(error|x) = min(P(Y=1|x),P(Y=-1|x))$

%The loss function $L(\alpha_i,\omega_j)$ is the 'loss' incurred for taking action %$\alpha_i$ instead of $\omega_j$

\end{homeworkSection}



\begin{homeworkSection}{\homeworkProblemName: (b)} % Section within problem
	\problemAnswer{
		Bayes risk: $R(f) = E[l(yf(x))]$ where $l(t) = I\{t \leq 0\}$
		
		For 0-1 loss, the conditional risk is:
		
		
		$R(i|x) = \sum_{j=0}^{k-1} L(j,i)p(j|x) \sum_{j \neq i} p(j|x) = 1-p(i|x)$
		
		
		Thus, $R(i|x)$ is the probability of $x$ not belonging to class $i$
		
		
		Bayes classifier found in previous part $g^*(x)$ is essentially: $g^{*}(x) = arg\ min_i R(i|x) = arg max_i p(i|x)$ which gave us the $\pi_+ p_+(x) \geq \pi_i p_-(x)$
		
		
		Conditional risk of Bayes classifier(binary):
		$R(g^*(x)|x) = min(1-p(0|x),1-p(1|x) = min(p(0|x), p(1|x))$
		
		and hence the bayes risk is given by:
		$R^* = \int min(p(0|x),p(1|x)) dF(x) $
		
		and hence in this case,
		$R^* = \int min(\pi_+ p_+(x), \pi_ip_-(x))dx $
		}
\end{homeworkSection}

\begin{homeworkSection}{\homeworkProblemName: (c)} % Section within problem
\problemAnswer{
	\begin{align*}
	\pi_+ = \frac{\sum_{i=1}^n I \{Y_i=1\}}{n}\\
  \pi_- = \frac{\sum_{i=1}^n I \{Y_i=-1\}}{n}
  \end{align*}
  We need to know $p(dx|Y=1)$ and $p(dx|Y=-1)$ so we need estimators for $a_+$, $a_-$, $\sigma$ and $\sum$
  
  We can choose MLE estimators for multivariate guassian(derivation skipped):
  
  \begin{align*}
  \hat{(a_+)} = \frac{\sum I\{Y_i=1\}}{n}\\
  \hat{(a_-)} = \frac{\sum I\{Y_i=-1\}}{n}\\
  \hat{\sigma} = \frac{\sum_{i;Y_i=1} (x_i-\hat{a_+})(x_i-\hat{a_+})^T}{\sum I\{Y_i=1\}}\\
  \hat{\sum} = \frac{\sum_{i;Y_i=-1} (x_i-\hat{a_-})(x_i-\hat{a_-})^T}{\sum I\{Y_i=1\}}\\
  \end{align*}
  
  And a suitable estimator of bayes classifier is :
  
  \begin{align*}
  \hat{g_*(x)} = \begin{cases}
  1 & \hat{\pi_+}\hat{p_+(x)} \geq \hat{\pi_{-}}\hat{p_-(x)}\\
  -1 & \text{otherwise}
  \end{cases}
  \end{align*}
  
  where $d$ is given by:
  \begin{align*}
  \log(\hat{\pi_+})  + \sqrt{\frac{1}{2\pi}} \mathbf{\hat{\sigma}}^{-1} (x-\vec{\hat{a_+}})\mathbf{\hat{\sigma}}^{-1}(x-\vec{\hat{a_+}})^T \geq \log(\hat{pi_-}) + \sqrt{\frac{1}{2\pi}} \mathbf{\hat{\sum}}^{-1} (x-\hat{{a_-}})\mathbf{\hat{\sum}}^{-1}(x-\hat{a_-})^T
\end{align*}
  
  
 	}
\end{homeworkSection}

\end{homeworkProblem}

\begin{homeworkProblem}[Problem 2]
	
\begin{homeworkSection}{\homeworkProblemName: (a)} % Section within problem
	\problemAnswer{
		$F(x)$ is 3 times differentiable.
		Consider taylor expansion of $F(x+h)$ and $F(x-h)$
		\begin{align*}
		F(x+h) = F(x) + F'(x)h + F''(x)\frac{h^2}{2} + F'''(x)\frac{h^3}{6} 
		\end{align*}
		
		\begin{align*}
		F(x-h) = F(x) - F'(x)h + F''(x)\frac{h^2}{2} - F'''(x)\frac{h^3}{6} 
		\end{align*}
		
		Thus,
		\begin{align*}
		F(x+h)-F(x-h) = 2F'(x)h + 2F'''(x)\frac{h^3}{6} \\
		\frac{F(x+h)-F(x-h)}{2h} = F'(x) + F'''(\epsilon)\frac{h^2}{12}\text{ for some $\epsilon$ in [x-h,x+h]}\\
		|F'(x)-\frac{F(x+h)-F(x-h)}{2h}| \leq |F'''(\epsilon)\frac{h^2}{12}|
		\end{align*}
		
	}
\end{homeworkSection}

\begin{homeworkSection}{\homeworkProblemName: (b)} % Section within problem
	\problemAnswer{
		Nadaraya-Watson Estimator $\eta_\eta(x) = E[Y|X=x] = \frac{\int yf(x,y)dy}{\int f(x,y)dy}$
		
		Now,
		 
		$f(x,y) = \frac{1}{nh_xh_y} \sum_{i=1}^n K(\frac{x-x_i}{h_x}) \times K(\frac{y-y_i}{h_y})$
		
		\begin{align*}
		\int yf(x,y) dy = \frac{1}{n} \int y \sum_{i=1}^n \frac{1}{h_xh_y} K(\frac{x-x_i}{h_x}) \times K(\frac{y-y_i}{h_y})
		\end{align*}
		
		
		
		Now, $\int y \frac{1}{h_y} K(\frac{y-y_i}{h_y})dy = y$
		
		Hence,
		\begin{align}
				\label{2a}
				\int yf(x,y) dy = \frac{1}{nh_x} \sum_{i=1}^n K(\frac{x-x_i})y_i
		\end{align}

		
		Consider $\int f(x,y) dy$:
		
		\begin{align}
		\label{2b}
		\int f(x,y) dy &= \frac{1}{nh_x} \sum_{i=1}^n K(\frac{x-x_i}{h_x}) \times \int K(\frac{y-y_i}{h_y})dy\\
		&= \frac{1}{nh_x} \sum_{i=1}^n K(\frac{x-x_i}{h_x}) \times 1 \text{ since  $\int K_{h_y} dy =1$}
		\end{align}
		
		Thus,
		using \ref{2a}, \ref{2a} we get:
		
		\begin{align*}
		\eta_\eta(x) = \frac{\frac{1}{nh_x} \sum_{i=1}^n K(\frac{x-x_i})y_i}{\frac{1}{nh_x} \sum_{i=1}^n K(\frac{x-x_i}{h_x})}\\
		\eta_\eta(x) = \frac{\sum_{i=1}^n y_iK(\frac{x-x_i})}{ \sum_{i=1}^n K(\frac{x-x_i}{h_x})}
		\end{align*}
	}
		
\end{homeworkSection}
\end{homeworkProblem}

\begin{homeworkProblem}[Problem 3]
	\problemAnswer{
		$L(g) := P(Y \neq g(X)) = E[I\{Yf(X)<0\}]$
		Consider $f(y)= max(1-y,0)$
		
		$L(g) = E[I\{Yf(X)<0\}] = E[E[max(1-f(x),0)|x=x]] = \int_S max(1-f(x),0) \times \frac{1-\eta(x)}{2} + max(1+f(x),0) \times \frac{1+\eta(x),2} d\pi$
		
		We need to minimise the integrand: $max(1-f(x),0) \times \frac{1-\eta(x)}{2} + max(1+f(x),0) \times \frac{1+\eta(x)}{2}$ 
		
			%	$ \frac{\eta(x)}{2}(max(1+f(x),0)-max(1-f(x),0) + \frac{1}{2}(max(1+f(x),0)+max(1-f(x),0))$ 
		
		%Minimise:
		
		%	$ \frac{\eta(x)}{2}(max(1+f(x),0)-max(-f(x),-1) + \frac{1}{2}(max(1+f(x),0)+max(1-f(x),0))$ 
		
		%Minimise:
			
			%$ \frac{\eta(x)}{2}(max(1+f(x),0)+max(f(x),1) + \frac{1}{2}(max(1+f(x),0)+max(1-f(x),0))$ 

		Which is to minimise: $max(1+tf(x))(1+t\eta(x))$ and is given by:
		
		$f(x)= sign(\eta(x)) = g^*(x)$ [Bayes classifier] 
			

		}
\end{homeworkProblem}	

\begin{homeworkProblem}[Problem 4]
	\problemAnswer{Assume the sufficient condition exists, i.e.:
		There exist $a_1,a_2, \dots a_k \geq 0$ and binary classifiers $g_1, g_2, \dots g_k$ such that $\forall 1 \leq i \leq n$:
		$Y_i\sum_{j=1}^k a_jg_j(X_i) \geq 2\gamma$
		
		
		$Y_i$ is given by the weighted sum of predictions $g_j$ : $Y_i = sign(\sum_{j=1}^k a_jg_j(X_i))$
		Taking expectations:
		\begin{align*}
		E[Y_i\sum_{j=1}^k a_jg_j(X_i)] \geq 2\gamma \\
		\sum_{j=1}^k a_j E[Y_ig_j(X_i)] \geq 2\gamma
		\end{align*}
		
		Since, $\sum_j a_j =1$ and $a_j \geq 0$ for $j= \{1,2,\dots,k\}$ and $\sum_{j=1}^k a_j E[Y_ig_j(X_i)] \geq 2\gamma$ then there exists a $g_j$ such that:
		
		$E[Y_ig_j(X_i)] \geq 2\gamma$
		
		\begin{align*}
		E[Y_ig_j(X_i)] &= 1\times P[Y_i=g_j(X_i)] + -1 \times P(Y_i \neq g_j(X_i))\\
		&= 1- 2P(Y_i \neq  g_j(X_i))\\
		\implies P(Y_i \neq  g_j(X_i)) = \frac{1-E[Y_ig_j(X_i)]}{2} \\
		P(Y_i \neq  g_j(X_i)) \leq \frac{1-2\gamma}{2}
		\end{align*}
		
		Now for weights, $w_1, w_2, \dots w_j$ such that $\sum_j w_j=1$:
		
		\begin{align*}
		\sum_{j=1}^n P(Y_j \neq g(X-j)) \leq \frac{1}{2}-\gamma \\
		\sum_{j=1}^n E[I(Y_j \neq g(X-j))] \leq \frac{1}{2}-\gamma \\
		\sum_{j=1}^n I(Y_j \neq g(X-j)) \leq \frac{1}{2}-\gamma \\
		\end{align*}
		}
\end{homeworkProblem}
\end{document}
