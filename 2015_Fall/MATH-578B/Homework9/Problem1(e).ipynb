{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "\n",
    "def simulate_genome(length, alpha):\n",
    "    read_arr = np.random.choice([0,1], length, p=[1-alpha, alpha])\n",
    "    reads = ''\n",
    "    for x in read_arr:\n",
    "        reads +=str(x)\n",
    "    return reads\n",
    "\n",
    "def countoccurences(word, read):\n",
    "    return read.count(word)\n",
    "\n",
    "NN = 100000\n",
    "L = 100\n",
    "alpha = 0.5\n",
    "max_iterations = 1000\n",
    "word = '101'\n",
    "nn = 100000\n",
    "c = NN*L/nn\n",
    "\n",
    "counts = {}\n",
    "#for n in range(1, nn):\n",
    "counts[nn] = []\n",
    "for i in range(1, max_iterations):\n",
    "    genome = simulate_genome(nn, alpha)\n",
    "    read_start_positions = np.random.choice(nn-L-1, NN)\n",
    "    occurences = 0\n",
    "    for p in read_start_positions:\n",
    "        read = genome[p:p+L]\n",
    "        occurences += countoccurences(word, read)\n",
    "    counts[nn].append(occurences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters used:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\alpha &= 0.5\\\\\n",
    "\\beta &= 0.5\\\\\n",
    "N &= 10000\\\\\n",
    "n &= 100000\\\\\n",
    "L &= 100\\\\\n",
    "c &= 100\\\\\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$\\lim_{n \\rightarrow \\infty} \\frac{Var(Y_n)}{n} = c^2\\frac{\\alpha^2\\beta(\\alpha+\\beta-3\\alpha^2\\beta+2\\alpha\\beta^2+2\\alpha\\beta+2\\beta^2)}{(\\alpha+\\beta)^2}$$\n",
    "\n",
    "$$\n",
    "\\lim_{n \\rightarrow \\infty} \\frac{Y_n}{n} = c\\frac{\\alpha^2\\beta}{\\alpha+\\beta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = alpha\n",
    "b = 1-alpha\n",
    "var = c**2 * ((a**2)*b *(a+b-3*(a**2)*b+2*a*(b**2)+2*a*b+2*b**2))/(nn*(a+b)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yn/n\t Simulated:9.84286574575 \t Analytical:12.5\n"
     ]
    }
   ],
   "source": [
    "print 'Yn/n\\t Simulated:{} \\t Analytical:{}'.format(np.mean(counts[nn])/nn,c*alpha**2*(1-alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Var(Yn)/n\t Simulated=0.0061746138811\t Analytical:0.0234375\n"
     ]
    }
   ],
   "source": [
    "print 'Var(Yn)/n\\t Simulated={}\\t Analytical:{}'.format(np.var(counts[nn])/nn**2, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, as seen from above, our analytical and simulated estimate of $\\frac{Y_n}{n}$ are 12.83 and 9.83 respectively. I used 1000 iterations, so the bound can improve by increasing the iterations. Similarly the estimate for $\\frac{Var(Y_n)}{n}$ is pretty close (simulated: 0.006, analytical: 0.02)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
